{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:25:22.648426Z","iopub.execute_input":"2025-08-24T17:25:22.649010Z","iopub.status.idle":"2025-08-24T17:25:22.655907Z","shell.execute_reply.started":"2025-08-24T17:25:22.648983Z","shell.execute_reply":"2025-08-24T17:25:22.654915Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install open-clip-torch\n!pip install faiss-cpu\n!pip install faiss-gpu\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:25:22.657223Z","iopub.execute_input":"2025-08-24T17:25:22.657447Z","iopub.status.idle":"2025-08-24T17:27:01.180694Z","shell.execute_reply.started":"2025-08-24T17:25:22.657425Z","shell.execute_reply":"2025-08-24T17:27:01.179923Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os, cv2, faiss, torch, numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom IPython.display import display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:27:01.181681Z","iopub.execute_input":"2025-08-24T17:27:01.182007Z","iopub.status.idle":"2025-08-24T17:27:06.126252Z","shell.execute_reply.started":"2025-08-24T17:27:01.181968Z","shell.execute_reply":"2025-08-24T17:27:06.125250Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"BACKEND = os.environ.get(\"MM_BACKEND\", \"siglip2\")  # \"openclip\" | \"siglip2\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif BACKEND == \"openclip\":\n    import open_clip\n    OC_MODEL = \"ViT-g-14\"\n    OC_PRETRAINED = \"laion2b_s34b_b88k\"\n    model, _, preprocess = open_clip.create_model_and_transforms(\n        OC_MODEL, pretrained=OC_PRETRAINED, device=DEVICE\n    )\n    tokenizer = open_clip.get_tokenizer(OC_MODEL)\n    model.eval()\n\nelif BACKEND == \"siglip2\":\n    # pip install -U transformers accelerate bitsandbytes\n    from transformers import AutoProcessor, AutoModel\n    CKPT = os.environ.get(\"SIGLIP2_CKPT\", \"google/siglip2-giant-opt-patch16-384\")\n    processor = AutoProcessor.from_pretrained(CKPT)\n    model = AutoModel.from_pretrained(CKPT, device_map=\"auto\").eval()\nelse:\n    raise ValueError(\"BACKEND phải là 'openclip' hoặc 'siglip2'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:27:06.127181Z","iopub.execute_input":"2025-08-24T17:27:06.127552Z","iopub.status.idle":"2025-08-24T17:28:47.247182Z","shell.execute_reply.started":"2025-08-24T17:27:06.127528Z","shell.execute_reply":"2025-08-24T17:28:47.246104Z"}},"outputs":[{"name":"stderr","text":"2025-08-24 17:27:17.613981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756056437.924997      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756056438.014423      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9c35b7094b47f7b9715d1484db9ef7"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd4e2dd058f46328e35541e2d67f4d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5282f5f065c14b76acf2f69cf4e9691a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc982442a9124a3189d919171fbba1e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dc81b688474ef39607d5ee75f37026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0fd52d4ecb44d9ab2f40e756075478f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1936af999f3543b69cf6619c355f8d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e38e26d6ac040e58294990545d95740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45f8aca1777437d82b0f69257b3222a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a999ea57bb46289fc4fb7529bdf8dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0846aaa110a1423e851a71719611ad9d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"@torch.no_grad()\ndef embed_images(image_paths, batch_size=64):\n    \"\"\"\n    frames: list[np.ndarray(H,W,3) in RGB]\n    return: np.ndarray [N, D] đã L2-norm\n    \"\"\"\n    if BACKEND == \"openclip\":\n        vecs = []\n        for i in range(0, len(image_paths), batch_size):\n            batch = [preprocess(Image.open(path)) for path in image_paths[i:i+batch_size]]\n            batch = torch.stack(batch).to(DEVICE)\n            feats = model.encode_image(batch)\n            feats = feats / feats.norm(dim=-1, keepdim=True)\n            vecs.append(feats.float().cpu().numpy())\n        return np.vstack(vecs)\n\n    elif BACKEND == \"siglip2\":\n        from PIL import Image as _Image\n        vecs = []\n        pil_frames = [Image.open(path) for path in image_paths]\n        for i in range(0, len(pil_frames), batch_size):\n            batch = pil_frames[i:i+batch_size]\n            inputs = processor(images=batch, return_tensors=\"pt\").to(model.device)\n            img_feats = model.get_image_features(**inputs)  # (B, D)\n            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n            vecs.append(img_feats.float().cpu().numpy())\n        return np.vstack(vecs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:45:36.564605Z","iopub.execute_input":"2025-08-24T17:45:36.564992Z","iopub.status.idle":"2025-08-24T17:45:36.573383Z","shell.execute_reply.started":"2025-08-24T17:45:36.564966Z","shell.execute_reply":"2025-08-24T17:45:36.572518Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"@torch.no_grad()\ndef embed_query_vi(text, en_hint=None):\n    \"\"\"\n    Trả về vector 1xD đã L2-norm; nếu có en_hint -> lấy max giữa 2 biến thể.\n    \"\"\"\n    if BACKEND == \"openclip\":\n        texts = [text] + ([en_hint] if en_hint else [])\n        toks = tokenizer(texts).to(DEVICE)\n        feats = model.encode_text(toks)\n        feats = feats / feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]\n\n    elif BACKEND == \"siglip2\":\n        texts = [text.lower()] + ([en_hint.lower()] if en_hint else [])\n        inputs = processor(\n            text=texts, return_tensors=\"pt\",\n            padding=\"max_length\", max_length=128\n        ).to(model.device)\n        txt_feats = model.get_text_features(**inputs)\n        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(txt_feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T18:20:10.935252Z","iopub.execute_input":"2025-08-24T18:20:10.935786Z","iopub.status.idle":"2025-08-24T18:20:10.942092Z","shell.execute_reply.started":"2025-08-24T18:20:10.935764Z","shell.execute_reply":"2025-08-24T18:20:10.941373Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ======= FAISS =======\ndef build_index(vecs: np.ndarray):\n    faiss.normalize_L2(vecs) \n    idx = faiss.IndexFlatIP(vecs.shape[1])\n    idx.add(vecs)\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:28:56.844135Z","iopub.execute_input":"2025-08-24T17:28:56.844471Z","iopub.status.idle":"2025-08-24T17:28:57.881333Z","shell.execute_reply.started":"2025-08-24T17:28:56.844440Z","shell.execute_reply":"2025-08-24T17:28:57.880464Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ======= SEARCH =======\ndef search_frames(image_paths, query_vi, query_en_hint=None,\n                  topk=10, save_dir=\"hits\", show=True):\n    os.makedirs(save_dir, exist_ok=True)\n\n    print(f\"Số frame sample: {len(image_paths)}\")\n    if not image_paths:\n        return []\n\n    print(\"Nhúng ảnh...\")\n    img_vecs = embed_images(image_paths)\n    index = build_index(img_vecs)\n\n    print(\"Nhúng truy vấn...\")\n    qv = embed_query_vi(query_vi, query_en_hint)\n\n    print(\"Tìm top-k...\")\n    D, I = index.search(qv.astype(\"float32\"), topk)\n    I, D = I[0].tolist(), D[0].tolist()\n\n    base = os.path.splitext(os.path.basename(image_paths[0]))[0]\n    results = []\n    for rank, (idx, score) in enumerate(zip(I, D), 1):\n        thumb = Image.open(image_paths[idx])\n        out = os.path.join(save_dir, f\"{base}_rank{rank:02d}_score{score:.3f}.jpg\")\n        thumb.save(out, quality=92)\n        results.append({\n            \"rank\": rank, \"similarity\": float(score),\n            \"thumb\": out, \"frame_index\": int(idx)\n        })\n        if show:\n            print(f\"Rank {rank} | score {score:.3f} | {out}\")\n            display(thumb)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:28:57.882098Z","iopub.execute_input":"2025-08-24T17:28:57.882415Z","iopub.status.idle":"2025-08-24T17:28:58.651501Z","shell.execute_reply.started":"2025-08-24T17:28:57.882387Z","shell.execute_reply":"2025-08-24T17:28:58.650414Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:28:58.652590Z","iopub.execute_input":"2025-08-24T17:28:58.652899Z","iopub.status.idle":"2025-08-24T17:28:59.747441Z","shell.execute_reply.started":"2025-08-24T17:28:58.652874Z","shell.execute_reply":"2025-08-24T17:28:59.746297Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"file_path = '/kaggle/input/aic-small-2024/Keyframes_L21/keyframes/L21_V001'\nimage_names = os.listdir(file_path)\nimage_paths = []\nfor i in range(len(image_names)):\n    image_paths.append(os.path.join(file_path, image_names[i]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T17:28:59.749173Z","iopub.execute_input":"2025-08-24T17:28:59.750160Z","iopub.status.idle":"2025-08-24T17:29:00.533576Z","shell.execute_reply.started":"2025-08-24T17:28:59.750120Z","shell.execute_reply":"2025-08-24T17:29:00.532508Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    query_vi = \"Nhiều người mặc áo xanh dương trong phòng thí nghiệm\"\n\n    # Tìm kiếm các ảnh phù hợp với truy vấn:\n    hits = search_frames(\n        image_paths, query_vi, topk=10, save_dir=\"hits\"\n    )\n\n    for h in hits:\n        print(f\"Rank {h['rank']} | score {h['similarity']:.3f}\")\n        img = Image.open(h[\"thumb\"])\n        display(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T18:20:18.840108Z","iopub.execute_input":"2025-08-24T18:20:18.841141Z","iopub.status.idle":"2025-08-24T18:22:40.725235Z","shell.execute_reply.started":"2025-08-24T18:20:18.841094Z","shell.execute_reply":"2025-08-24T18:22:40.723921Z"}},"outputs":[{"name":"stdout","text":"Số frame sample: 307\nNhúng ảnh...\nNhúng truy vấn...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4114288750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Tìm kiếm các ảnh phù hợp với truy vấn:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     hits = search_frames(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_vi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hits\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     )\n","\u001b[0;32m/tmp/ipykernel_36/3147648903.py\u001b[0m in \u001b[0;36msearch_frames\u001b[0;34m(image_paths, query_vi, query_en_hint, topk, save_dir, show)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nhúng truy vấn...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mqv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_query_vi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_en_hint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tìm top-k...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/409401088.py\u001b[0m in \u001b[0;36membed_query_vi\u001b[0;34m(text, en_hint)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         ).to(model.device)\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtxt_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtxt_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_feats\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtxt_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/siglip/modeling_siglip.py\u001b[0m in \u001b[0;36mget_text_features\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    966\u001b[0m         )\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         text_outputs: BaseModelOutputWithPooling = self.text_model(\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/siglip/modeling_siglip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# note: SigLIP's text model does not use a causal mask, unlike the original CLIP model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/siglip/modeling_siglip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_position_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    331\u001b[0m                 \u001b[0;34mf\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;34mf\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Sequence length must be less than max_position_embeddings (got `sequence length`: 128 and max_position_embeddings: 64"],"ename":"ValueError","evalue":"Sequence length must be less than max_position_embeddings (got `sequence length`: 128 and max_position_embeddings: 64","output_type":"error"}],"execution_count":23}]}