{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:39:19.300237Z","iopub.execute_input":"2025-08-24T15:39:19.301104Z","iopub.status.idle":"2025-08-24T15:39:19.307020Z","shell.execute_reply.started":"2025-08-24T15:39:19.301066Z","shell.execute_reply":"2025-08-24T15:39:19.306456Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers accelerate\n!pip install qwen-vl-utils[decord]==0.0.8\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:39:19.308236Z","iopub.execute_input":"2025-08-24T15:39:19.308445Z","iopub.status.idle":"2025-08-24T15:41:15.004822Z","shell.execute_reply.started":"2025-08-24T15:39:19.308420Z","shell.execute_reply":"2025-08-24T15:41:15.004041Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# default processer\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:41:15.006256Z","iopub.execute_input":"2025-08-24T15:41:15.006464Z","iopub.status.idle":"2025-08-24T15:44:46.297812Z","shell.execute_reply.started":"2025-08-24T15:41:15.006442Z","shell.execute_reply":"2025-08-24T15:44:46.297147Z"}},"outputs":[{"name":"stderr","text":"2025-08-24 15:41:29.869718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756050090.228123      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756050090.332070      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212eb0f51b0442d4a9eebade9f25f6ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f7cbbd426064981bf481adb26e45dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acdd3d9f328e4658819c71c2682264b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70972b1f4a7c42d1994d5acac65c7940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4480cddd298444ad8f4a839208e5accf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44561eaeda814643b4f0ee0a9097732a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845ef72d920244bfa592ce3900599043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ead36c8fba495d9cd41eab3f780901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0f11a169c74c7aaf08790fe5e5cda0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5672491cc0f4725a542235b750836c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031bf129f7c44822a05e99f80bd38b34"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2426a9d1124a3d8edf6a0fd8f85e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7994ae2fd8a7451bb2d445e67be5d6f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8612503df7743caa7d2ef67af5f69d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a1ad0ab34343d29bb873a8a6709df2"}},"metadata":{}},{"name":"stderr","text":"You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b2164a16984b70b89c8871a0ffc8d8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:44:46.298592Z","iopub.execute_input":"2025-08-24T15:44:46.299174Z","iopub.status.idle":"2025-08-24T15:44:46.303238Z","shell.execute_reply.started":"2025-08-24T15:44:46.299154Z","shell.execute_reply":"2025-08-24T15:44:46.302565Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def infer_batch(image_paths: list[str], df: pd.DataFrame):\n    \"\"\"\n    Thực hiện OCR batch cho nhiều ảnh bằng model đã load sẵn.\n\n    Args:\n        image_paths (list[str]): danh sách đường dẫn ảnh cần OCR.\n\n    Returns:\n        list[str]: list các chuỗi văn bản OCR tương ứng với từng ảnh.\n    \"\"\"\n    # Tạo danh sách messages cho từng ảnh\n    all_messages = []\n    for img in image_paths:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"image\": img,\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"\"\"Extract all text that is clearly visible and fully legible in the image.\n                        Crucially, only transcribe text that is completely unobstructed, uncovered, and not partially hidden by any other objects or elements within the image.\n                        Do not infer, guess, or hallucinate any text that is unclear, obscured, or not genuinely present.\n                        Only output text that is definitively and entirely legible.\n                        Present the extracted text line by line.\"\"\"\n                    },\n                ],\n            }\n        ] \n        all_messages.append(messages)\n\n    # Chuẩn bị input batch\n    texts = [\n        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n        for msg in all_messages\n    ]\n    image_inputs, video_inputs = process_vision_info(all_messages)\n\n    inputs = processor(\n        text=texts,\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    ).to(\"cuda\")\n\n    # Batch inference\n    generated_ids = model.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=False,\n        temperature=0.0,\n        repetition_penalty=1.0,\n    )\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_texts = processor.batch_decode(\n        generated_ids_trimmed,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False,\n    )\n\n    batch_df = pd.DataFrame({\n            \"image_path\": image_paths,\n            \"text_recognition\": [txt if txt else \"EMPTY\" for txt in output_texts]\n        })\n\n        # Dùng pd.concat để gộp batch với df hiện tại\n    df = pd.concat([df, batch_df], ignore_index=True)\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:44:46.304910Z","iopub.execute_input":"2025-08-24T15:44:46.305196Z","iopub.status.idle":"2025-08-24T15:44:46.325135Z","shell.execute_reply.started":"2025-08-24T15:44:46.305177Z","shell.execute_reply":"2025-08-24T15:44:46.324521Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\n\nfile_name = os.listdir('/kaggle/input/aic-small-2024/Keyframes_L22/keyframes/L22_V002')\nfile_name = file_name[:22]\nlist_imgs = []\nfor i in range(len(file_name)):\n    list_imgs.append(os.path.join('/kaggle/input/aic-small-2024/Keyframes_L22/keyframes/L22_V002', file_name[i]))\n\ndf = pd.DataFrame(columns=[\"image_path\", \"text_recognition\"])\n\nbatch_size = 10\nstart = time.time()\n\nfor i in range(0, len(list_imgs), batch_size):\n    batch_imgs = list_imgs[i:i+batch_size] \n    df = infer_batch(batch_imgs, df) \n\nend = time.time()\n\n# In ra thời gian thực hiện\nprint(\"Time taken: \", end - start)\n\n# Sau khi xử lý tất cả ảnh, bạn có thể lưu kết quả vào file CSV\ndf.to_csv('ocr_output.csv', mode='a', header=False, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:44:46.325871Z","iopub.execute_input":"2025-08-24T15:44:46.326103Z","iopub.status.idle":"2025-08-24T15:56:02.493083Z","shell.execute_reply.started":"2025-08-24T15:44:46.326068Z","shell.execute_reply":"2025-08-24T15:56:02.492443Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Time taken:  676.0610826015472\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df['text_recognition'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:56:02.494021Z","iopub.execute_input":"2025-08-24T15:56:02.494282Z","iopub.status.idle":"2025-08-24T15:56:02.526579Z","shell.execute_reply.started":"2025-08-24T15:56:02.494263Z","shell.execute_reply":"2025-08-24T15:56:02.526019Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'HTV7\\n18:31:38\\nThs. NGUYỄN VĂN\\nTRƯỞNG PHÒNG\\nĐiện Biên nỗ lực đảm bảo giao thông mùa mưa lũ\\nLâm Đồn'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T15:56:02.527339Z","iopub.execute_input":"2025-08-24T15:56:02.527883Z","iopub.status.idle":"2025-08-24T15:56:02.574788Z","shell.execute_reply.started":"2025-08-24T15:56:02.527854Z","shell.execute_reply":"2025-08-24T15:56:02.574202Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                          image_path  \\\n0  /kaggle/input/aic-small-2024/Keyframes_L22/key...   \n1  /kaggle/input/aic-small-2024/Keyframes_L22/key...   \n2  /kaggle/input/aic-small-2024/Keyframes_L22/key...   \n3  /kaggle/input/aic-small-2024/Keyframes_L22/key...   \n4  /kaggle/input/aic-small-2024/Keyframes_L22/key...   \n\n                                    text_recognition  \n0                                     HTV7\\n18:44:34  \n1  HTV7\\n18:31:38\\nThs. NGUYỄN VĂN\\nTRƯỞNG PHÒNG\\...  \n2  NGÀY HỘI VIỆC LÀM\\nTTYT QUẬN 8 TUYỂN DỤNG\\n20 ...  \n3  HTV7\\n18:49:47\\nBÌNH THUẬN: CHÌM TÀU CÁ, 6 NGƯ...  \n4                                  HTV7 HD\\n18:46:16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text_recognition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/aic-small-2024/Keyframes_L22/key...</td>\n      <td>HTV7\\n18:44:34</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/aic-small-2024/Keyframes_L22/key...</td>\n      <td>HTV7\\n18:31:38\\nThs. NGUYỄN VĂN\\nTRƯỞNG PHÒNG\\...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/aic-small-2024/Keyframes_L22/key...</td>\n      <td>NGÀY HỘI VIỆC LÀM\\nTTYT QUẬN 8 TUYỂN DỤNG\\n20 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/aic-small-2024/Keyframes_L22/key...</td>\n      <td>HTV7\\n18:49:47\\nBÌNH THUẬN: CHÌM TÀU CÁ, 6 NGƯ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/aic-small-2024/Keyframes_L22/key...</td>\n      <td>HTV7 HD\\n18:46:16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8}]}