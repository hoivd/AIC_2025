{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12779183,"sourceType":"datasetVersion","datasetId":8079103},{"sourceId":12858514,"sourceType":"datasetVersion","datasetId":8129235},{"sourceId":545126,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":418001,"modelId":435667}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/hein-nkhh/unilm.git\n%cd unilm/beit3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-04T13:59:11.312480Z","iopub.execute_input":"2025-09-04T13:59:11.313743Z","iopub.status.idle":"2025-09-04T13:59:15.924853Z","shell.execute_reply.started":"2025-09-04T13:59:11.313708Z","shell.execute_reply":"2025-09-04T13:59:15.923986Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'unilm'...\nremote: Enumerating objects: 11122, done.\u001b[K\nremote: Counting objects: 100% (43/43), done.\u001b[K\nremote: Compressing objects: 100% (25/25), done.\u001b[K\nremote: Total 11122 (delta 31), reused 18 (delta 18), pack-reused 11079 (from 4)\u001b[K\nReceiving objects: 100% (11122/11122), 75.39 MiB | 33.80 MiB/s, done.\nResolving deltas: 100% (5248/5248), done.\n/kaggle/working/unilm/beit3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T13:59:15.926736Z","iopub.execute_input":"2025-09-04T13:59:15.926967Z","iopub.status.idle":"2025-09-04T13:59:15.930878Z","shell.execute_reply.started":"2025-09-04T13:59:15.926945Z","shell.execute_reply":"2025-09-04T13:59:15.930306Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -r requirements.txt\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T13:59:15.931479Z","iopub.execute_input":"2025-09-04T13:59:15.931744Z","iopub.status.idle":"2025-09-04T14:01:06.776947Z","shell.execute_reply.started":"2025-09-04T13:59:15.931722Z","shell.execute_reply":"2025-09-04T14:01:06.776197Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nfrom modeling_finetune import beit3_large_patch16_384_retrieval\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast\nfrom transformers import XLMRobertaTokenizer\nimport torch\nimport json\nimport cv2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:01:06.778628Z","iopub.execute_input":"2025-09-04T14:01:06.778849Z","iopub.status.idle":"2025-09-04T14:01:21.450883Z","shell.execute_reply.started":"2025-09-04T14:01:06.778825Z","shell.execute_reply":"2025-09-04T14:01:21.450273Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer(\"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3.spm\")\n\n# M√¥ h√¨nh beit_3\nckpt = \"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3_large_patch16_384_coco_retrieval.pth\"\nmodel = beit3_large_patch16_384_retrieval(pretrained=False)\nstate_dict = torch.load(ckpt, map_location=device)\nmodel.load_state_dict(state_dict[\"model\"], strict=False)\nmodel = model.to(device)\nmodel.eval()\nclear_output()\n\ntransform  = transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:01:21.451654Z","iopub.execute_input":"2025-09-04T14:01:21.452025Z","iopub.status.idle":"2025-09-04T14:01:40.000664Z","shell.execute_reply.started":"2025-09-04T14:01:21.452007Z","shell.execute_reply":"2025-09-04T14:01:39.999874Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# M√¥ h√¨nh translate\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"VietAI/envit5-translation\"\ntokenizer_translate = AutoTokenizer.from_pretrained(model_name)  \nmodel_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel_translate = model_translate.to('cuda:0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:01:40.001636Z","iopub.execute_input":"2025-09-04T14:01:40.001884Z","iopub.status.idle":"2025-09-04T14:02:07.947505Z","shell.execute_reply.started":"2025-09-04T14:01:40.001863Z","shell.execute_reply":"2025-09-04T14:02:07.946549Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaaaf76023f14bc69dbe45116f2471c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9756151a1fed4b258b566cf02f583fe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b79370700e64f1384a6647e4777d827"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c9d4e0e0074735b0c0e9e48f21f9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0cd4ea6f08848e18f2683bedbee6036"}},"metadata":{}},{"name":"stderr","text":"2025-09-04 14:01:46.011545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756994506.333539      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756994506.428279      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"360e7200c8cc4dee820eceb8e0f6fd13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6351d36d3aac4225a577dccd3d6371be"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def encode_text(query: str, tokenizer, model, max_len=32, device=device):\n    tokens = tokenizer.tokenize(query)\n    if len(tokens) > max_len - 2:\n        tokens = tokens[:max_len - 2]\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    bos, eos, pad = tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id\n    token_ids = [bos] + token_ids + [eos]\n    num_tokens = len(token_ids)\n\n    padding_mask = [0] * num_tokens + [1] * (max_len - num_tokens)\n    language_tokens = token_ids + [pad] * (max_len - num_tokens)\n\n    language_tokens = torch.tensor(language_tokens, dtype=torch.long).unsqueeze(0).to(device)\n    padding_mask = torch.tensor(padding_mask, dtype=torch.long).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        _, language_cls = model(text_description=language_tokens,\n                                padding_mask=padding_mask,\n                                only_infer=True)\n        lang_norm = F.normalize(language_cls, p=2, dim=-1).cpu()  # (1,D)\n    return lang_norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:07.948570Z","iopub.execute_input":"2025-09-04T14:02:07.952269Z","iopub.status.idle":"2025-09-04T14:02:07.978892Z","shell.execute_reply.started":"2025-09-04T14:02:07.952231Z","shell.execute_reply":"2025-09-04T14:02:07.976046Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# json_files = ['/kaggle/input/aic-sample-test/keyframes_index/L21_V003_keyframes_index.json',\n#              # '/kaggle/input/aic-sample-test/keyframes_index/L21_V006_keyframes_index.json',\n#              # '/kaggle/input/aic-sample-test/keyframes_index/L21_V007_keyframes_index.json',\n#              # '/kaggle/input/aic-sample-test/keyframes_index/L21_V011_keyframes_index.json'\n#             ]\n\n\n# video_paths = ['/kaggle/input/aic-sample-test/videos/L21_V003.mp4', \n#                # '/kaggle/input/aic-sample-test/videos/L21_V006.mp4',\n#                # '/kaggle/input/aic-sample-test/videos/L21_V007.mp4',\n#                # '/kaggle/input/aic-sample-test/videos/L21_V011.mp4'\n#               ]\n\n# output_dir = 'extracted_frames'\n# os.makedirs(output_dir, exist_ok=True)\n\n# image_paths = []\n\n# for i, json_file in enumerate(json_files):\n#     with open(json_file, 'r') as f:\n#         frame_indices = json.load(f)\n\n#     cap = cv2.VideoCapture(video_paths[i])\n    \n#     if not cap.isOpened():\n#         print(f\"Kh√¥ng th·ªÉ m·ªü video: {video_paths[i]}\")\n#         continue\n\n#     # tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n#     for frame_index in tqdm(frame_indices, desc=f\"Tr√≠ch xu·∫•t t·ª´ video {i+1}/{len(json_files)}\"):\n#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n#         ret, frame = cap.read()\n#         if not ret:\n#             print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc frame t·∫°i index {frame_index} trong video {i+1}.\")\n#             continue\n\n#         frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#         frame_image = Image.fromarray(frame_rgb)\n        \n#         frame_path = os.path.join(output_dir, f'video{i+1}_frame_{frame_index}.png')\n#         frame_image.save(frame_path)\n#         image_paths.append(frame_path)\n\n#     cap.release()\n    \n# clear_output()\n# print(f\"ƒê√£ tr√≠ch xu·∫•t t·ªïng c·ªông {len(image_paths)} frame.\")\n# # print(f\"C√°c ƒë∆∞·ªùng d·∫´n frame:\", image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:07.979748Z","iopub.execute_input":"2025-09-04T14:02:07.980108Z","iopub.status.idle":"2025-09-04T14:02:08.020171Z","shell.execute_reply.started":"2025-09-04T14:02:07.980075Z","shell.execute_reply":"2025-09-04T14:02:08.017749Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# image_paths = sorted(image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:08.021410Z","iopub.execute_input":"2025-09-04T14:02:08.027194Z","iopub.status.idle":"2025-09-04T14:02:08.050312Z","shell.execute_reply.started":"2025-09-04T14:02:08.027160Z","shell.execute_reply":"2025-09-04T14:02:08.049332Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# embeddings = []\n# ids = []\n\n# with torch.no_grad():\n#     for img_path in tqdm(image_paths, desc=\"üîÑ Extracting image embeddings\"):\n#         image = Image.open(img_path).convert(\"RGB\")\n#         image_tensor = transform(image).unsqueeze(0).to(device)\n\n#         with autocast(): \n#             vision_cls, _ = model(image=image_tensor, only_infer=True)\n#             vision_norm = F.normalize(vision_cls, p=2, dim=-1)\n\n#         embeddings.append(vision_norm.squeeze(0).cpu())   # (D,)\n#         ids.append(img_path)\n\n#         del image_tensor, vision_cls, vision_norm\n#         torch.cuda.empty_cache()\n\n# image_embeddings = torch.stack(embeddings, dim=0)  # (N,D)\n# torch.save({\"embeddings\": image_embeddings, \"ids\": ids}, \"image_embeddings.pt\")\n# print(\"‚úÖ Saved embeddings:\", image_embeddings.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:08.057003Z","iopub.execute_input":"2025-09-04T14:02:08.060772Z","iopub.status.idle":"2025-09-04T14:02:08.072008Z","shell.execute_reply.started":"2025-09-04T14:02:08.060739Z","shell.execute_reply":"2025-09-04T14:02:08.068535Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    force=True,                 # √©p ghi ƒë√® c·∫•u h√¨nh c≈© (r·∫•t quan tr·ªçng trong notebook)\n)\n\nlogger = logging.getLogger(\"Embedd Frame\")\nlogger.info(\"Xin ch√†o\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:33.137750Z","iopub.execute_input":"2025-09-04T14:02:33.138020Z","iopub.status.idle":"2025-09-04T14:02:33.143579Z","shell.execute_reply.started":"2025-09-04T14:02:33.138000Z","shell.execute_reply":"2025-09-04T14:02:33.142911Z"}},"outputs":[{"name":"stderr","text":"2025-09-04 14:02:33,139 - Embedd Frame - INFO - Xin ch√†o\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def extract_embeddings_batch(image_paths, model, transform, device, batch_size=16):\n    \"\"\"\n    Extract embeddings in batches for faster processing\n    \n    Args:\n        image_paths: List of image file paths\n        model: Vision model\n        transform: Image preprocessing transform\n        device: Device to run on\n        batch_size: Number of images to process at once\n    \"\"\"\n    embeddings = []\n    ids = []\n    \n    # Process images in batches\n    with torch.no_grad():\n        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"üîÑ Extracting embeddings (batched)\"):\n            batch_paths = image_paths[i:i + batch_size]\n            \n            # Load and preprocess batch of images\n            batch_images = []\n            batch_ids = []\n            \n            for img_path in batch_paths:\n                try:\n                    # image = Image.open(img_path).convert(\"RGB\")\n                    image_tensor = transform(img_path)\n                    batch_images.append(image_tensor)\n                    batch_ids.append(img_path)\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n                    continue\n            \n            if not batch_images:\n                continue\n                \n            # Stack into batch tensor\n            batch_tensor = torch.stack(batch_images).to(device)  # (B, C, H, W)\n            \n            # Extract features for the batch\n            with autocast():\n                vision_cls_batch = model(image=batch_tensor, only_infer=True)[0]  # (B, D)\n                vision_norm_batch = F.normalize(vision_cls_batch, p=2, dim=-1)\n            \n            # Add to results\n            embeddings.extend([emb.cpu() for emb in vision_norm_batch])\n            ids.extend(batch_ids)\n            \n            # Clean up memory\n            del batch_tensor, vision_cls_batch, vision_norm_batch\n            torch.cuda.empty_cache()\n    \n    # Stack all embeddings\n    image_embeddings = torch.stack(embeddings, dim=0)  # (N, D)\n    \n    return image_embeddings, ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:06:29.919062Z","iopub.execute_input":"2025-09-04T14:06:29.919853Z","iopub.status.idle":"2025-09-04T14:06:29.926977Z","shell.execute_reply.started":"2025-09-04T14:06:29.919819Z","shell.execute_reply":"2025-09-04T14:06:29.926375Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def extract_frames_with_opencv(\n    video_path: str, \n    target_height: int = 27, \n    target_width: int = 48, \n    target_fps: float = None,        # th√™m tu·ª≥ ch·ªçn fps\n    show_progressbar: bool = False\n):\n    \"\"\"\n    Extracts frames from a video using OpenCV and returns a list of PIL Images.\n    If target_fps is set, frames will be sampled to match that FPS.\n    \"\"\"\n    logger.info(f\"Opening video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        logger.error(f\"Failed to open video: {video_path}\")\n        raise ValueError(f\"Failed to open video: {video_path}\")\n\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # N·∫øu c√≥ target_fps th√¨ t√≠nh step\n    if target_fps is not None and target_fps > 0 and video_fps > 0:\n        step = int(round(video_fps / target_fps))\n        logger.info(f\"Video FPS: {video_fps:.2f}, target FPS: {target_fps}, step: {step}\")\n    else:\n        step = 1\n        logger.info(f\"Video FPS: {video_fps:.2f}, using all frames\")\n\n    frames = []\n\n    progress_bar = tqdm(total=total_frames, desc=\"Extracting frames\", unit=\"frame\") if show_progressbar else None\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx % step == 0:   # ch·ªâ l·∫•y frame theo step\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_resized = cv2.resize(frame_rgb, (target_width, target_height))\n            img_pil = Image.fromarray(frame_resized)\n            frames.append(img_pil)\n\n        frame_idx += 1\n        if progress_bar:\n            progress_bar.update(1)\n\n    cap.release()\n    if progress_bar:\n        progress_bar.close()\n    logger.info(f\"Extracted {len(frames)} frames (from {total_frames})\")\n    return frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:09.134243Z","iopub.execute_input":"2025-09-04T14:02:09.135160Z","iopub.status.idle":"2025-09-04T14:02:11.490421Z","shell.execute_reply.started":"2025-09-04T14:02:09.135109Z","shell.execute_reply":"2025-09-04T14:02:11.489373Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"image_paths = extract_frames_with_opencv('/kaggle/input/aic-sample-test/videos/L21_V001.mp4',\n                                         show_progressbar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:36.710745Z","iopub.execute_input":"2025-09-04T14:02:36.711538Z","iopub.status.idle":"2025-09-04T14:04:06.911939Z","shell.execute_reply.started":"2025-09-04T14:02:36.711485Z","shell.execute_reply":"2025-09-04T14:04:06.911332Z"}},"outputs":[{"name":"stderr","text":"2025-09-04 14:02:36,712 - Embedd Frame - INFO - Opening video: /kaggle/input/aic-sample-test/videos/L21_V001.mp4\n2025-09-04 14:02:36,895 - Embedd Frame - INFO - Video FPS: 30.00, using all frames\nExtracting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37849/37849 [01:30<00:00, 420.50frame/s]\n2025-09-04 14:04:06,908 - Embedd Frame - INFO - Extracted 37849 frames (from 37849)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"embeddings, ids = extract_embeddings_batch(\n    image_paths=image_paths,\n    model=model,\n    transform=transform,\n    device=device,\n    batch_size=64  # Adjust based on your GPU memory\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:06:35.912520Z","iopub.execute_input":"2025-09-04T14:06:35.913014Z"}},"outputs":[{"name":"stderr","text":"üîÑ Extracting embeddings (batched):   0%|          | 0/592 [00:00<?, ?it/s]/tmp/ipykernel_36/2964938504.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nüîÑ Extracting embeddings (batched):  18%|‚ñà‚ñä        | 106/592 [07:22<34:25,  4.25s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save({\"embeddings\": embeddings, \"ids\": ids}, \"image_embeddings.pt\")\nprint(\"‚úÖ Saved embeddings:\", embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:12.855289Z","iopub.status.idle":"2025-09-04T14:02:12.855542Z","shell.execute_reply.started":"2025-09-04T14:02:12.855407Z","shell.execute_reply":"2025-09-04T14:02:12.855417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_vi_en(text: str) -> str:\n    # Th√™m prefix \"vi:\" v√†o input\n    prefixed = f\"vi: {text}\"\n    inputs = tokenizer_translate([prefixed], return_tensors=\"pt\", padding=True).input_ids.to('cuda:0')\n    outputs = model_translate.generate(inputs, max_length=50)\n    translated = tokenizer_translate.decode(outputs[0], skip_special_tokens=True)\n    # X√≥a prefix \"en:\" n·∫øu c√≥\n    return translated.replace(\"en:\", \"\").strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:12.857071Z","iopub.status.idle":"2025-09-04T14:02:12.857870Z","shell.execute_reply.started":"2025-09-04T14:02:12.857683Z","shell.execute_reply":"2025-09-04T14:02:12.857700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_query = \"BƒÉng tan ·ªü Nam C·ª±c\"\ntext_query_en = translate_vi_en(text_query)\nlang_vec = encode_text(text_query_en, tokenizer, model)\n\n# Similarity\nsimilarities = torch.matmul(lang_vec, embeddings.t()).squeeze(0)  # (N,)\n\n# Top-K results\ntopk = torch.topk(similarities, k=10)\nprint(\"Top matches:\")\nfor idx, score in zip(topk.indices, topk.values):\n    img_path = ids[idx]\n    print(img_path, float(score))\n\n    img = Image.open(img_path)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Sim: {score:.4f}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T14:02:12.858912Z","iopub.status.idle":"2025-09-04T14:02:12.859218Z","shell.execute_reply.started":"2025-09-04T14:02:12.859050Z","shell.execute_reply":"2025-09-04T14:02:12.859066Z"}},"outputs":[],"execution_count":null}]}