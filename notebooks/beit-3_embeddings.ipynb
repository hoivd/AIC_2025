{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12779183,"sourceType":"datasetVersion","datasetId":8079103},{"sourceId":12858514,"sourceType":"datasetVersion","datasetId":8129235},{"sourceId":545126,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":418001,"modelId":435667}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/hein-nkhh/unilm.git\n%cd unilm/beit3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:41:37.618825Z","iopub.execute_input":"2025-08-27T09:41:37.619341Z","iopub.status.idle":"2025-08-27T09:41:42.230199Z","shell.execute_reply.started":"2025-08-27T09:41:37.619309Z","shell.execute_reply":"2025-08-27T09:41:42.229416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:41:42.231977Z","iopub.execute_input":"2025-08-27T09:41:42.232296Z","iopub.status.idle":"2025-08-27T09:41:42.235897Z","shell.execute_reply.started":"2025-08-27T09:41:42.232265Z","shell.execute_reply":"2025-08-27T09:41:42.235301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r requirements.txt\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:41:42.236655Z","iopub.execute_input":"2025-08-27T09:41:42.236830Z","iopub.status.idle":"2025-08-27T09:43:35.609657Z","shell.execute_reply.started":"2025-08-27T09:41:42.236815Z","shell.execute_reply":"2025-08-27T09:43:35.608933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nfrom modeling_finetune import beit3_large_patch16_384_retrieval\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast\nfrom transformers import XLMRobertaTokenizer\nimport torch\nimport json\nimport cv2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:43:35.610511Z","iopub.execute_input":"2025-08-27T09:43:35.610773Z","iopub.status.idle":"2025-08-27T09:43:50.505193Z","shell.execute_reply.started":"2025-08-27T09:43:35.610738Z","shell.execute_reply":"2025-08-27T09:43:50.504623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer(\"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3.spm\")\n\n# M√¥ h√¨nh beit_3\nckpt = \"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3_large_patch16_384_coco_retrieval.pth\"\nmodel = beit3_large_patch16_384_retrieval(pretrained=False)\nstate_dict = torch.load(ckpt, map_location=device)\nmodel.load_state_dict(state_dict[\"model\"], strict=False)\nmodel = model.to(device)\nmodel.eval()\nclear_output()\n\ntransform  = transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:43:50.506806Z","iopub.execute_input":"2025-08-27T09:43:50.507166Z","iopub.status.idle":"2025-08-27T09:44:15.148407Z","shell.execute_reply.started":"2025-08-27T09:43:50.507150Z","shell.execute_reply":"2025-08-27T09:44:15.147769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# M√¥ h√¨nh translate\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"VietAI/envit5-translation\"\ntokenizer_translate = AutoTokenizer.from_pretrained(model_name)  \nmodel_translate = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel_translate = model_translate.to('cuda:0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:44:15.149105Z","iopub.execute_input":"2025-08-27T09:44:15.149386Z","iopub.status.idle":"2025-08-27T09:44:44.555159Z","shell.execute_reply.started":"2025-08-27T09:44:15.149350Z","shell.execute_reply":"2025-08-27T09:44:44.554303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_text(query: str, tokenizer, model, max_len=32, device=device):\n    tokens = tokenizer.tokenize(query)\n    if len(tokens) > max_len - 2:\n        tokens = tokens[:max_len - 2]\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    bos, eos, pad = tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id\n    token_ids = [bos] + token_ids + [eos]\n    num_tokens = len(token_ids)\n\n    padding_mask = [0] * num_tokens + [1] * (max_len - num_tokens)\n    language_tokens = token_ids + [pad] * (max_len - num_tokens)\n\n    language_tokens = torch.tensor(language_tokens, dtype=torch.long).unsqueeze(0).to(device)\n    padding_mask = torch.tensor(padding_mask, dtype=torch.long).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        _, language_cls = model(text_description=language_tokens,\n                                padding_mask=padding_mask,\n                                only_infer=True)\n        lang_norm = F.normalize(language_cls, p=2, dim=-1).cpu()  # (1,D)\n    return lang_norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:44:44.555966Z","iopub.execute_input":"2025-08-27T09:44:44.557914Z","iopub.status.idle":"2025-08-27T09:44:44.573713Z","shell.execute_reply.started":"2025-08-27T09:44:44.557883Z","shell.execute_reply":"2025-08-27T09:44:44.571569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_files = ['/kaggle/input/aic-sample-test/keyframes_index/L21_V003_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V006_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V007_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V011_keyframes_index.json'\n            ]\n\n\nvideo_paths = ['/kaggle/input/aic-sample-test/videos/L21_V003.mp4', \n               '/kaggle/input/aic-sample-test/videos/L21_V006.mp4',\n               '/kaggle/input/aic-sample-test/videos/L21_V007.mp4',\n               '/kaggle/input/aic-sample-test/videos/L21_V011.mp4'\n              ]\n\noutput_dir = 'extracted_frames'\nos.makedirs(output_dir, exist_ok=True)\n\nimage_paths = []\n\nfor i, json_file in enumerate(json_files):\n    with open(json_file, 'r') as f:\n        frame_indices = json.load(f)\n\n    cap = cv2.VideoCapture(video_paths[i])\n    \n    if not cap.isOpened():\n        print(f\"Kh√¥ng th·ªÉ m·ªü video: {video_paths[i]}\")\n        continue\n\n    # tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n    for frame_index in tqdm(frame_indices, desc=f\"Tr√≠ch xu·∫•t t·ª´ video {i+1}/{len(json_files)}\"):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc frame t·∫°i index {frame_index} trong video {i+1}.\")\n            continue\n\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame_image = Image.fromarray(frame_rgb)\n        \n        frame_path = os.path.join(output_dir, f'video{i+1}_frame_{frame_index}.png')\n        frame_image.save(frame_path)\n        image_paths.append(frame_path)\n\n    cap.release()\n    \nclear_output()\nprint(f\"ƒê√£ tr√≠ch xu·∫•t t·ªïng c·ªông {len(image_paths)} frame.\")\n# print(f\"C√°c ƒë∆∞·ªùng d·∫´n frame:\", image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:44:44.576577Z","iopub.execute_input":"2025-08-27T09:44:44.576902Z","iopub.status.idle":"2025-08-27T09:52:15.104515Z","shell.execute_reply.started":"2025-08-27T09:44:44.576871Z","shell.execute_reply":"2025-08-27T09:52:15.103768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_paths = sorted(image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:54:46.049356Z","iopub.execute_input":"2025-08-27T09:54:46.049675Z","iopub.status.idle":"2025-08-27T09:54:46.054143Z","shell.execute_reply.started":"2025-08-27T09:54:46.049653Z","shell.execute_reply":"2025-08-27T09:54:46.053315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# path = \"/kaggle/input/aic-small-2024/Keyframes_L21/keyframes/L21_V001\"\n# image_paths = [os.path.join(path, name) for name in os.listdir(path)]\n\nembeddings = []\nids = []\n\nwith torch.no_grad():\n    for img_path in tqdm(image_paths, desc=\"üîÑ Extracting image embeddings\"):\n        image = Image.open(img_path).convert(\"RGB\")\n        image_tensor = transform(image).unsqueeze(0).to(device)\n\n        with autocast(): \n            vision_cls, _ = model(image=image_tensor, only_infer=True)\n            vision_norm = F.normalize(vision_cls, p=2, dim=-1)\n\n        embeddings.append(vision_norm.squeeze(0).cpu())   # (D,)\n        ids.append(img_path)\n\n        del image_tensor, vision_cls, vision_norm\n        torch.cuda.empty_cache()\n\nimage_embeddings = torch.stack(embeddings, dim=0)  # (N,D)\ntorch.save({\"embeddings\": image_embeddings, \"ids\": ids}, \"image_embeddings.pt\")\nprint(\"‚úÖ Saved embeddings:\", image_embeddings.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:54:58.702826Z","iopub.execute_input":"2025-08-27T09:54:58.703155Z","iopub.status.idle":"2025-08-27T09:58:05.905917Z","shell.execute_reply.started":"2025-08-27T09:54:58.703133Z","shell.execute_reply":"2025-08-27T09:58:05.904889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_vi_en(text: str) -> str:\n    # Th√™m prefix \"vi:\" v√†o input\n    prefixed = f\"vi: {text}\"\n    inputs = tokenizer_translate([prefixed], return_tensors=\"pt\", padding=True).input_ids.to('cuda:0')\n    outputs = model_translate.generate(inputs, max_length=50)\n    translated = tokenizer_translate.decode(outputs[0], skip_special_tokens=True)\n    # X√≥a prefix \"en:\" n·∫øu c√≥\n    return translated.replace(\"en:\", \"\").strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:59:19.921661Z","iopub.execute_input":"2025-08-27T09:59:19.922018Z","iopub.status.idle":"2025-08-27T09:59:19.928298Z","shell.execute_reply.started":"2025-08-27T09:59:19.921995Z","shell.execute_reply":"2025-08-27T09:59:19.927301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_query = \"BƒÉng tan ·ªü Nam C·ª±c\"\ntext_query_en = translate_vi_en(text_query)\nlang_vec = encode_text(text_query_en, tokenizer, model)\n\n# Similarity\nsimilarities = torch.matmul(lang_vec, image_embeddings.t()).squeeze(0)  # (N,)\n\n# Top-K results\ntopk = torch.topk(similarities, k=10)\nprint(\"Top matches:\")\nfor idx, score in zip(topk.indices, topk.values):\n    img_path = ids[idx]\n    print(img_path, float(score))\n\n    img = Image.open(img_path)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Sim: {score:.4f}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:59:21.742193Z","iopub.execute_input":"2025-08-27T09:59:21.742976Z","iopub.status.idle":"2025-08-27T09:59:26.109287Z","shell.execute_reply.started":"2025-08-27T09:59:21.742941Z","shell.execute_reply":"2025-08-27T09:59:26.108322Z"}},"outputs":[],"execution_count":null}]}