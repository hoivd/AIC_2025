{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12779183,"sourceType":"datasetVersion","datasetId":8079103},{"sourceId":12858514,"sourceType":"datasetVersion","datasetId":8129235}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:18:29.349761Z","iopub.execute_input":"2025-08-25T12:18:29.350570Z","iopub.status.idle":"2025-08-25T12:18:29.358586Z","shell.execute_reply.started":"2025-08-25T12:18:29.350534Z","shell.execute_reply":"2025-08-25T12:18:29.357768Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install open-clip-torch\n!pip install faiss-cpu\n!pip install faiss-gpu\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:18:29.360283Z","iopub.execute_input":"2025-08-25T12:18:29.360555Z","iopub.status.idle":"2025-08-25T12:19:54.537793Z","shell.execute_reply.started":"2025-08-25T12:18:29.360532Z","shell.execute_reply":"2025-08-25T12:19:54.536840Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os, cv2, faiss, torch, numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom IPython.display import display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:19:54.538945Z","iopub.execute_input":"2025-08-25T12:19:54.539625Z","iopub.status.idle":"2025-08-25T12:19:58.451843Z","shell.execute_reply.started":"2025-08-25T12:19:54.539595Z","shell.execute_reply":"2025-08-25T12:19:58.451093Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"BACKEND = os.environ.get(\"MM_BACKEND\", \"openclip\")  # \"openclip\" | \"siglip2\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif BACKEND == \"openclip\":\n    import open_clip\n    OC_MODEL = \"ViT-g-14\"\n    OC_PRETRAINED = \"laion2b_s34b_b88k\"\n    model, _, preprocess = open_clip.create_model_and_transforms(\n        OC_MODEL, pretrained=OC_PRETRAINED, device=DEVICE\n    )\n    tokenizer = open_clip.get_tokenizer(OC_MODEL)\n    model.eval()\n\nelif BACKEND == \"siglip2\":\n    # pip install -U transformers accelerate bitsandbytes\n    from transformers import AutoProcessor, AutoModel\n    CKPT = os.environ.get(\"SIGLIP2_CKPT\", \"google/siglip2-giant-opt-patch16-384\")\n    processor = AutoProcessor.from_pretrained(CKPT)\n    model = AutoModel.from_pretrained(CKPT, device_map=\"auto\").eval()\nelse:\n    raise ValueError(\"BACKEND phải là 'openclip' hoặc 'siglip2'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:19:58.453853Z","iopub.execute_input":"2025-08-25T12:19:58.454267Z","iopub.status.idle":"2025-08-25T12:21:33.404987Z","shell.execute_reply.started":"2025-08-25T12:19:58.454247Z","shell.execute_reply":"2025-08-25T12:21:33.404052Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 12:20:04.897392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756124405.112894      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756124405.177005      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f69beab3a848c58ca8f001c0e9c27c"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb0d4ce2ce84682af66e8cc693ae9b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3586cc66613143c1b4fcbd98911953fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c645b139a3cf4d9aa11098e4d0a3f1ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d1ef1066c744e8b81edb8d3a15430f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7e03b1275748c1b9874e8207efada2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d97f315b0a6411c92797a14874c6121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8532b90bfcc143e1a173c183b9f8abf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"515100bd23a740fea64b59e9bb7979f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374a4d485ed741379f0f27b84e9ddde2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3feeac2f3894d2892555170660043a7"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"@torch.no_grad()\ndef embed_images(image_paths, batch_size=64):\n    \"\"\"\n    frames: list[np.ndarray(H,W,3) in RGB]\n    return: np.ndarray [N, D] đã L2-norm\n    \"\"\"\n    if BACKEND == \"openclip\":\n        vecs = []\n        for i in range(0, len(image_paths), batch_size):\n            batch = [preprocess(Image.open(path)) for path in image_paths[i:i+batch_size]]\n            batch = torch.stack(batch).to(DEVICE)\n            feats = model.encode_image(batch)\n            feats = feats / feats.norm(dim=-1, keepdim=True)\n            vecs.append(feats.float().cpu().numpy())\n        return np.vstack(vecs)\n\n    elif BACKEND == \"siglip2\":\n        from PIL import Image as _Image\n        vecs = []\n        pil_frames = [Image.open(path) for path in image_paths]\n        for i in range(0, len(pil_frames), batch_size):\n            batch = pil_frames[i:i+batch_size]\n            inputs = processor(images=batch, return_tensors=\"pt\").to(model.device)\n            img_feats = model.get_image_features(**inputs)  # (B, D)\n            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n            vecs.append(img_feats.float().cpu().numpy())\n        return np.vstack(vecs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:33.405787Z","iopub.execute_input":"2025-08-25T12:21:33.406782Z","iopub.status.idle":"2025-08-25T12:21:33.419560Z","shell.execute_reply.started":"2025-08-25T12:21:33.406745Z","shell.execute_reply":"2025-08-25T12:21:33.418116Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"@torch.no_grad()\ndef embed_query_vi(text, en_hint=None):\n    \"\"\"\n    Trả về vector 1xD đã L2-norm; nếu có en_hint -> lấy max giữa 2 biến thể.\n    \"\"\"\n    if BACKEND == \"openclip\":\n        texts = [text] + ([en_hint] if en_hint else [])\n        toks = tokenizer(texts).to(DEVICE)\n        feats = model.encode_text(toks)\n        feats = feats / feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]\n\n    elif BACKEND == \"siglip2\":\n        texts = [text.lower()] + ([en_hint.lower()] if en_hint else [])\n        inputs = processor(\n            text=texts, return_tensors=\"pt\",\n            padding=\"max_length\", max_length=64\n        ).to(model.device)\n        txt_feats = model.get_text_features(**inputs)\n        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(txt_feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:33.420226Z","iopub.execute_input":"2025-08-25T12:21:33.420494Z","iopub.status.idle":"2025-08-25T12:21:39.830833Z","shell.execute_reply.started":"2025-08-25T12:21:33.420471Z","shell.execute_reply":"2025-08-25T12:21:39.829940Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ======= FAISS =======\ndef build_index(vecs: np.ndarray):\n    faiss.normalize_L2(vecs) \n    idx = faiss.IndexFlatIP(vecs.shape[1])\n    idx.add(vecs)\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:39.831664Z","iopub.execute_input":"2025-08-25T12:21:39.832228Z","iopub.status.idle":"2025-08-25T12:21:46.323016Z","shell.execute_reply.started":"2025-08-25T12:21:39.832200Z","shell.execute_reply":"2025-08-25T12:21:46.322230Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# # ======= SEARCH =======\n# def search_frames(image_paths, query_vi, query_en_hint=None,\n#                   topk=10, save_dir=\"hits\", show=True):\n#     os.makedirs(save_dir, exist_ok=True)\n\n#     print(f\"Số frame sample: {len(image_paths)}\")\n#     if not image_paths:\n#         return []\n\n#     print(\"Nhúng ảnh...\")\n#     img_vecs = embed_images(image_paths)\n#     index = build_index(img_vecs)\n\n#     print(\"Nhúng truy vấn...\")\n#     qv = embed_query_vi(query_vi, query_en_hint)\n\n#     print(\"Tìm top-k...\")\n#     D, I = index.search(qv.astype(\"float32\"), topk)\n#     I, D = I[0].tolist(), D[0].tolist()\n\n#     base = os.path.splitext(os.path.basename(image_paths[0]))[0]\n#     results = []\n#     for rank, (idx, score) in enumerate(zip(I, D), 1):\n#         thumb = Image.open(image_paths[idx])\n#         out = os.path.join(save_dir, f\"{base}_rank{rank:02d}_score{score:.3f}.jpg\")\n#         thumb.save(out, quality=92)\n#         results.append({\n#             \"rank\": rank, \"similarity\": float(score),\n#             \"thumb\": out, \"frame_index\": int(idx)\n#         })\n#         if show:\n#             print(f\"Rank {rank} | score {score:.3f} | {out}\")\n#             display(thumb)\n\n#     return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.323865Z","iopub.execute_input":"2025-08-25T12:21:46.324174Z","iopub.status.idle":"2025-08-25T12:21:46.336682Z","shell.execute_reply.started":"2025-08-25T12:21:46.324146Z","shell.execute_reply":"2025-08-25T12:21:46.336026Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pickle\n\ndef save_image_embeddings(image_paths, save_path=\"image_embeddings.pkl\"):\n    print(f\"Lưu embeddings của {len(image_paths)} ảnh...\")\n    \n    # Nhúng ảnh\n    img_vecs = embed_images(image_paths)\n    \n    # Lưu embeddings vào file\n    with open(save_path, 'wb') as f:\n        pickle.dump(img_vecs, f)\n    \n    print(f\"Đã lưu embeddings vào {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.337373Z","iopub.execute_input":"2025-08-25T12:21:46.337617Z","iopub.status.idle":"2025-08-25T12:21:46.353610Z","shell.execute_reply.started":"2025-08-25T12:21:46.337601Z","shell.execute_reply":"2025-08-25T12:21:46.353020Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef search_frames_with_saved_embeddings(image_paths, query_vi, query_en_hint=None, \n                                        topk=10, save_dir=\"hits\", embedding_path=\"image_embeddings.pkl\", show=True):\n    # Kiểm tra nếu file embeddings đã tồn tại\n    if not os.path.exists(embedding_path):\n        raise FileNotFoundError(f\"Không tìm thấy file embeddings: {embedding_path}\")\n    \n    # Đọc embeddings đã lưu\n    with open(embedding_path, 'rb') as f:\n        img_vecs = pickle.load(f)\n    \n    # Tạo chỉ mục cho embeddings đã lưu\n    index = build_index(img_vecs)\n\n    print(f\"Số frame sample: {len(image_paths)}\")\n    if not image_paths:\n        return []\n\n    print(\"Nhúng truy vấn...\")\n    qv = embed_query_vi(query_vi, query_en_hint)\n\n    print(\"Tìm top-k...\")\n    D, I = index.search(qv.astype(\"float32\"), topk)\n    I, D = I[0].tolist(), D[0].tolist()\n\n    base = os.path.splitext(os.path.basename(image_paths[0]))[0]\n    results = []\n    \n    os.makedirs(save_dir, exist_ok=True)\n    \n    for rank, (idx, score) in enumerate(zip(I, D), 1):\n        thumb = Image.open(image_paths[idx])\n        out = os.path.join(save_dir, f\"{base}_rank{rank:02d}_score{score:.3f}.jpg\")\n        thumb.save(out, quality=92)\n        results.append({\n            \"rank\": rank, \"similarity\": float(score),\n            \"thumb\": out, \"frame_index\": int(idx)\n        })\n        if show:\n            print(f\"Rank {rank} | score {score:.3f} | {out}\")\n            display(thumb)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.355646Z","iopub.execute_input":"2025-08-25T12:21:46.355837Z","iopub.status.idle":"2025-08-25T12:21:46.368240Z","shell.execute_reply.started":"2025-08-25T12:21:46.355822Z","shell.execute_reply":"2025-08-25T12:21:46.367584Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.368981Z","iopub.execute_input":"2025-08-25T12:21:46.369257Z","iopub.status.idle":"2025-08-25T12:21:46.386586Z","shell.execute_reply.started":"2025-08-25T12:21:46.369235Z","shell.execute_reply":"2025-08-25T12:21:46.385954Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"file_path = '/kaggle/input/aic-small-2024/Keyframes_L21/keyframes/L21_V001'\nimage_names = os.listdir(file_path)\nimage_paths = []\nfor i in range(len(image_names)):\n    image_paths.append(os.path.join(file_path, image_names[i]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.387251Z","iopub.execute_input":"2025-08-25T12:21:46.387546Z","iopub.status.idle":"2025-08-25T12:21:46.421486Z","shell.execute_reply.started":"2025-08-25T12:21:46.387529Z","shell.execute_reply":"2025-08-25T12:21:46.420822Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\nimport cv2\n\njson_files = ['/kaggle/input/aic-sample-test/keyframes_index/L21_V003_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V006_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V007_keyframes_index.json',\n             '/kaggle/input/aic-sample-test/keyframes_index/L21_V011_keyframes_index.json'\n            ]\n\n\nvideo_paths = ['/kaggle/input/aic-sample-test/videos/L21_V003.mp4', \n               '/kaggle/input/aic-sample-test/videos/L21_V006.mp4',\n               '/kaggle/input/aic-sample-test/videos/L21_V007.mp4',\n               '/kaggle/input/aic-sample-test/videos/L21_V011.mp4'\n              ] \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.422229Z","iopub.execute_input":"2025-08-25T12:21:46.422512Z","iopub.status.idle":"2025-08-25T12:21:46.426025Z","shell.execute_reply.started":"2025-08-25T12:21:46.422486Z","shell.execute_reply":"2025-08-25T12:21:46.425483Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"output_dir = 'extracted_frames'\nos.makedirs(output_dir, exist_ok=True)\n\nimage_paths = []\n\nfor i, json_file in enumerate(json_files):\n    with open(json_file, 'r') as f:\n        frame_indices = json.load(f)\n\n    cap = cv2.VideoCapture(video_paths[i])\n    \n    if not cap.isOpened():\n        print(f\"Không thể mở video: {video_paths[i]}\")\n        continue\n\n    # tqdm để hiển thị tiến trình\n    for frame_index in tqdm(frame_indices, desc=f\"Trích xuất từ video {i+1}/{len(json_files)}\"):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n        ret, frame = cap.read()\n        if not ret:\n            print(f\"⚠️ Không thể đọc frame tại index {frame_index} trong video {i+1}.\")\n            continue\n\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame_image = Image.fromarray(frame_rgb)\n        \n        frame_path = os.path.join(output_dir, f'video{i+1}_frame_{frame_index}.png')\n        frame_image.save(frame_path)\n        image_paths.append(frame_path)\n\n    cap.release()\n\nprint(f\"Đã trích xuất tổng cộng {len(image_paths)} frame.\")\nprint(f\"Các đường dẫn frame:\", image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:21:46.426729Z","iopub.execute_input":"2025-08-25T12:21:46.427069Z","execution_failed":"2025-08-25T12:25:51.882Z"}},"outputs":[{"name":"stderr","text":"Trích xuất từ video 1/4:   5%|▍         | 16/349 [00:03<01:32,  3.59it/s][h264 @ 0x3abe98c0] mmco: unref short failure\nTrích xuất từ video 1/4:   5%|▍         | 17/349 [00:04<01:21,  4.07it/s][h264 @ 0x3abe98c0] mmco: unref short failure\nTrích xuất từ video 1/4:   5%|▌         | 18/349 [00:04<01:14,  4.44it/s][h264 @ 0x3abe98c0] mmco: unref short failure\nTrích xuất từ video 1/4:  92%|█████████▏| 321/349 [01:36<00:08,  3.40it/s][h264 @ 0x3abe98c0] mmco: unref short failure\nTrích xuất từ video 1/4: 100%|██████████| 349/349 [01:43<00:00,  3.36it/s]\nTrích xuất từ video 2/4:  19%|█▉        | 72/384 [00:20<01:27,  3.58it/s][h264 @ 0x3ab5b1c0] mmco: unref short failure\nTrích xuất từ video 2/4:  36%|███▋      | 140/384 [00:43<01:21,  2.98it/s][h264 @ 0x3ab5b1c0] mmco: unref short failure\n[h264 @ 0x3ab5b1c0] mmco: unref short failure\nTrích xuất từ video 2/4:  42%|████▏     | 160/384 [00:49<00:49,  4.52it/s][h264 @ 0x3ab5b1c0] mmco: unref short failure\nTrích xuất từ video 2/4:  52%|█████▏    | 198/384 [01:00<01:20,  2.32it/s][h264 @ 0x3ab5b1c0] mmco: unref short failure\n[h264 @ 0x3ab5b1c0] mmco: unref short failure\nTrích xuất từ video 2/4:  76%|███████▌  | 291/384 [01:26<00:27,  3.35it/s][h264 @ 0x3ab5b1c0] mmco: unref short failure\nTrích xuất từ video 2/4: 100%|██████████| 384/384 [01:50<00:00,  3.46it/s]\nTrích xuất từ video 3/4:  32%|███▏      | 92/288 [00:29<01:07,  2.92it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"save_image_embeddings(image_paths, \"image_embeddings.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:25:51.882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query_vi = \"Băng tan ở Nam Cực\"\nresults = search_frames_with_saved_embeddings(image_paths, query_vi, topk=10, save_dir=\"hits\", embedding_path=\"image_embeddings.pkl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-25T12:25:51.882Z"}},"outputs":[],"execution_count":null}]}