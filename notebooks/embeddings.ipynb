{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12828263,"sourceType":"datasetVersion","datasetId":8112773}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os, cv2, faiss, torch, numpy as np\n# from PIL import Image\n# import open_clip\n# from tqdm import tqdm\n# from IPython.display import display\n\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model, _, preprocess = open_clip.create_model_and_transforms(\n#     \"ViT-g-14\", pretrained=\"laion2b_s34b_b88k\", device=DEVICE\n# )\n# tokenizer = open_clip.get_tokenizer(\"ViT-g-14\")  # <-- thêm dòng này\n# model.eval()\n\n# def sample_frames(video_path, every_ms=500, max_frames=None,\n#                   time_window=\"first_half\", start_sec=None, end_sec=None):\n#     \"\"\"\n#     time_window: \"first_half\" | \"second_half\" | None\n#     Hoặc chỉ định start_sec/end_sec (ưu tiên hơn time_window).\n#     \"\"\"\n#     cap = cv2.VideoCapture(video_path)\n#     assert cap.isOpened(), f\"Không mở được video: {video_path}\"\n#     fps = cap.get(cv2.CAP_PROP_FPS) or 25\n#     n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n#     dur = n_frames / fps if n_frames > 0 else None\n\n#     # Tính khoảng đọc\n#     if start_sec is None and end_sec is None and dur is not None:\n#         if time_window == \"first_half\":\n#             start_sec, end_sec = 0.0, dur * 0.5\n#         elif time_window == \"second_half\":\n#             start_sec, end_sec = dur * 0.5, dur\n#         else:\n#             start_sec, end_sec = 0.0, dur  # toàn bộ\n#     elif start_sec is None and end_sec is None:\n#         # không biết duration -> đọc toàn bộ (hoặc dùng max_frames)\n#         start_sec, end_sec = 0.0, float(\"inf\")\n\n#     # Seek nhanh tới điểm bắt đầu (nếu biết fps & frame_count)\n#     if dur is not None:\n#         start_frame = int(start_sec * fps)\n#         cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n\n#     step = max(1, int(fps * (every_ms/1000.0)))\n#     frames, times = [], []\n#     i = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n\n#     with tqdm(desc=\"Đọc video (có giới hạn)\", unit=\"f\") as pbar:\n#         while True:\n#             # Dừng nếu đã vượt end_sec\n#             cur_sec = cap.get(cv2.CAP_PROP_POS_MSEC)/1000.0\n#             if cur_sec and cur_sec > end_sec: break\n\n#             ret, fr = cap.read()\n#             if not ret: break\n#             if i % step == 0:\n#                 rgb = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n#                 frames.append(rgb)\n#                 times.append(cap.get(cv2.CAP_PROP_POS_MSEC)/1000.0)\n#                 if max_frames and len(frames) >= max_frames:\n#                     break\n#             i += 1\n#             pbar.update(1)\n\n#     cap.release()\n#     return frames, times\n\n# @torch.no_grad()\n# def embed_images(frames, batch_size=64):\n#     vecs = []\n#     for i in range(0, len(frames), batch_size):\n#         batch = [preprocess(Image.fromarray(x)) for x in frames[i:i+batch_size]]\n#         batch = torch.stack(batch).to(DEVICE)\n#         feats = model.encode_image(batch)\n#         feats = feats / feats.norm(dim=-1, keepdim=True)\n#         vecs.append(feats.float().cpu().numpy())\n#     return np.vstack(vecs)\n\n# @torch.no_grad()\n# def embed_query_vi(text, en_hint=None):\n#     texts = [text] + ([en_hint] if en_hint else [])\n#     toks = tokenizer(texts).to(DEVICE)\n#     feats = model.encode_text(toks)\n#     feats = feats / feats.norm(dim=-1, keepdim=True)\n#     feat = torch.max(feats, dim=0).values  # lấy biến thể tốt nhất\n#     return feat.float().cpu().numpy()[None, :]\n\n# def build_index(vecs):\n#     faiss.normalize_L2(vecs)\n#     idx = faiss.IndexFlatIP(vecs.shape[1])\n#     idx.add(vecs)\n#     return idx\n\n# def search_frames(video_path, query_vi, query_en_hint=None,\n#                   every_ms=500, topk=10, save_dir=\"hits\",\n#                   time_window=\"first_half\", start_sec=None, end_sec=None):\n#     os.makedirs(save_dir, exist_ok=True)\n#     frames, times = sample_frames(\n#         video_path, every_ms=every_ms, time_window=time_window,\n#         start_sec=start_sec, end_sec=end_sec\n#     )\n#     print(f\"Số frame sample: {len(frames)}\")\n#     if not frames: return []\n\n#     img_vecs = embed_images(frames)\n#     index = build_index(img_vecs)\n#     qv = embed_query_vi(query_vi, query_en_hint)\n\n#     D, I = index.search(qv, topk)\n#     I, D = I[0].tolist(), D[0].tolist()\n\n#     base = os.path.splitext(os.path.basename(video_path))[0]\n#     results = []\n#     for rank, (idx, score) in enumerate(zip(I, D), 1):\n#         t = times[idx]\n#         thumb = Image.fromarray(frames[idx])\n#         out = os.path.join(save_dir, f\"{base}_rank{rank:02d}_t{t:.2f}s_{score:.3f}.jpg\")\n#         thumb.save(out, quality=92)\n#         results.append({\n#             \"rank\": rank, \"time_sec\": float(t), \"similarity\": float(score),\n#             \"thumb\": out, \"frame_index\": int(idx)\n#         })\n#     return results\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:56:53.407990Z","iopub.execute_input":"2025-08-22T08:56:53.408475Z","iopub.status.idle":"2025-08-22T08:56:53.415843Z","shell.execute_reply.started":"2025-08-22T08:56:53.408454Z","shell.execute_reply":"2025-08-22T08:56:53.415220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport os, cv2, faiss, torch, numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n# ======= CHỌN BACKEND =======\n# \"openclip\" (như bạn đang dùng) hoặc \"siglip2\"\nBACKEND = os.environ.get(\"MM_BACKEND\", \"siglip2\")  # \"openclip\" | \"siglip2\"\n\n# ======= LOAD MODEL =======\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif BACKEND == \"openclip\":\n    import open_clip\n    OC_MODEL = \"ViT-g-14\"\n    OC_PRETRAINED = \"laion2b_s34b_b88k\"\n    model, _, preprocess = open_clip.create_model_and_transforms(\n        OC_MODEL, pretrained=OC_PRETRAINED, device=DEVICE\n    )\n    tokenizer = open_clip.get_tokenizer(OC_MODEL)\n    model.eval()\n\nelif BACKEND == \"siglip2\":\n    # pip install -U transformers accelerate bitsandbytes\n    from transformers import AutoProcessor, AutoModel\n    CKPT = os.environ.get(\"SIGLIP2_CKPT\", \"google/siglip2-giant-opt-patch16-384\")\n    processor = AutoProcessor.from_pretrained(CKPT)\n    model = AutoModel.from_pretrained(CKPT, device_map=\"auto\").eval()\nelse:\n    raise ValueError(\"BACKEND phải là 'openclip' hoặc 'siglip2'.\")\n\n# ======= VIDEO FRAME SAMPLER =======\ndef sample_frames(video_path, every_ms=500, max_frames=None,\n                  time_window=\"first_half\", start_sec=None, end_sec=None):\n    \"\"\"\n    time_window: \"first_half\" | \"second_half\" | None\n    Hoặc chỉ định start_sec/end_sec (ưu tiên hơn time_window).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    assert cap.isOpened(), f\"Không mở được video: {video_path}\"\n    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n    dur = (n_frames / fps) if n_frames > 0 else None\n\n    # Tính khoảng đọc\n    if start_sec is None and end_sec is None and dur is not None:\n        if time_window == \"first_half\":\n            start_sec, end_sec = 0.0, dur * 0.5\n        elif time_window == \"second_half\":\n            start_sec, end_sec = dur * 0.5, dur\n        else:\n            start_sec, end_sec = 0.0, dur  # toàn bộ\n    elif start_sec is None and end_sec is None:\n        start_sec, end_sec = 0.0, float(\"inf\")\n\n    # Seek tới frame bắt đầu (nếu biết)\n    if dur is not None:\n        start_frame = int(start_sec * fps)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n\n    step = max(1, int(fps * (every_ms / 1000.0)))\n    frames, times = [], []\n    i = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n\n    with tqdm(desc=\"Đọc video (giới hạn)\", unit=\"f\") as pbar:\n        while True:\n            # Kiểm tra dừng theo end_sec (nếu POS_MSEC khả dụng)\n            pos_msec = cap.get(cv2.CAP_PROP_POS_MSEC)\n            if pos_msec > 0:\n                cur_sec = pos_msec / 1000.0\n                if cur_sec > end_sec:\n                    break\n\n            ret, fr = cap.read()\n            if not ret:\n                break\n\n            if i % step == 0:\n                rgb = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n                frames.append(rgb)\n\n                # Lấy thời gian hiện tại sau khi read; fallback nếu =0\n                pos_msec2 = cap.get(cv2.CAP_PROP_POS_MSEC)\n                if pos_msec2 and pos_msec2 > 0:\n                    t_sec = pos_msec2 / 1000.0\n                else:\n                    # Fallback: ước lượng theo frame index\n                    t_sec = i / fps\n                times.append(t_sec)\n\n                if max_frames and len(frames) >= max_frames:\n                    break\n\n            i += 1\n            pbar.update(1)\n\n    cap.release()\n    return frames, times\n\n# ======= EMBEDDING FUNCS (tuỳ BACKEND) =======\n@torch.no_grad()\ndef embed_images(frames, batch_size=64):\n    \"\"\"\n    frames: list[np.ndarray(H,W,3) in RGB]\n    return: np.ndarray [N, D] đã L2-norm\n    \"\"\"\n    if BACKEND == \"openclip\":\n        vecs = []\n        for i in range(0, len(frames), batch_size):\n            batch = [preprocess(Image.fromarray(x)) for x in frames[i:i+batch_size]]\n            batch = torch.stack(batch).to(DEVICE)\n            feats = model.encode_image(batch)\n            feats = feats / feats.norm(dim=-1, keepdim=True)\n            vecs.append(feats.float().cpu().numpy())\n        return np.vstack(vecs)\n\n    elif BACKEND == \"siglip2\":\n        from PIL import Image as _Image\n        vecs = []\n        pil_frames = [_Image.fromarray(x) for x in frames]\n        for i in range(0, len(pil_frames), batch_size):\n            batch = pil_frames[i:i+batch_size]\n            inputs = processor(images=batch, return_tensors=\"pt\").to(model.device)\n            img_feats = model.get_image_features(**inputs)  # (B, D)\n            img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n            vecs.append(img_feats.float().cpu().numpy())\n        return np.vstack(vecs)\n\n@torch.no_grad()\ndef embed_query_vi(text, en_hint=None):\n    \"\"\"\n    Trả về vector 1xD đã L2-norm; nếu có en_hint -> lấy max giữa 2 biến thể.\n    \"\"\"\n    if BACKEND == \"openclip\":\n        texts = [text] + ([en_hint] if en_hint else [])\n        toks = tokenizer(texts).to(DEVICE)\n        feats = model.encode_text(toks)\n        feats = feats / feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]\n\n    elif BACKEND == \"siglip2\":\n        texts = [text.lower()] + ([en_hint.lower()] if en_hint else [])\n        inputs = processor(\n            text=texts, return_tensors=\"pt\",\n            padding=\"max_length\", max_length=64\n        ).to(model.device)\n        txt_feats = model.get_text_features(**inputs)\n        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n        feat = torch.max(txt_feats, dim=0).values\n        return feat.float().cpu().numpy()[None, :]\n\n# ======= FAISS =======\ndef build_index(vecs: np.ndarray):\n    faiss.normalize_L2(vecs)  # an toàn thêm\n    idx = faiss.IndexFlatIP(vecs.shape[1])\n    idx.add(vecs)\n    return idx\n\n# ======= SEARCH =======\ndef search_frames(video_path, query_vi, query_en_hint=None,\n                  every_ms=500, topk=10, save_dir=\"hits\",\n                  time_window=\"first_half\", start_sec=None, end_sec=None,\n                  show=True):\n    os.makedirs(save_dir, exist_ok=True)\n    frames, times = sample_frames(\n        video_path, every_ms=every_ms, time_window=time_window,\n        start_sec=start_sec, end_sec=end_sec\n    )\n    print(f\"Số frame sample: {len(frames)}\")\n    if not frames:\n        return []\n\n    print(\"Nhúng ảnh...\")\n    img_vecs = embed_images(frames)\n    index = build_index(img_vecs)\n\n    print(\"Nhúng truy vấn...\")\n    qv = embed_query_vi(query_vi, query_en_hint)\n\n    print(\"Tìm top-k...\")\n    D, I = index.search(qv.astype(\"float32\"), topk)\n    I, D = I[0].tolist(), D[0].tolist()\n\n    base = os.path.splitext(os.path.basename(video_path))[0]\n    results = []\n    for rank, (idx, score) in enumerate(zip(I, D), 1):\n        t = times[idx]\n        thumb = Image.fromarray(frames[idx])\n        out = os.path.join(save_dir, f\"{base}_rank{rank:02d}_t{t:.2f}s_{score:.3f}.jpg\")\n        thumb.save(out, quality=92)\n        results.append({\n            \"rank\": rank, \"time_sec\": float(t), \"similarity\": float(score),\n            \"thumb\": out, \"frame_index\": int(idx)\n        })\n        if show:\n            print(f\"Rank {rank} | {t:.2f}s | score {score:.3f} | {out}\")\n            display(thumb)\n\n    return results\n\n# # ======= DEMO (tuỳ chọn) =======\n# if __name__ == \"__main__\":\n#     video_path = \"sample.mp4\"\n#     query_vi = \"người đội mũ bảo hiểm đi xe máy trên đường\"\n#     query_en = \"a person wearing a helmet riding a motorbike on a street\"\n\n#     hits = search_frames(\n#         video_path, query_vi, query_en_hint=query_en,\n#         every_ms=500, topk=10, time_window=\"first_half\", show=False\n#     )\n#     print(\"\\nKết quả:\")\n#     for h in hits: print(h)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:13:28.177471Z","iopub.execute_input":"2025-08-22T09:13:28.177758Z","iopub.status.idle":"2025-08-22T09:15:09.194823Z","shell.execute_reply.started":"2025-08-22T09:13:28.177740Z","shell.execute_reply":"2025-08-22T09:15:09.194284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    video_path = \"/kaggle/input/video-small/L21_V001.mp4\"  # <-- đổi thành video của bạn\n    query_vi = \"Nhiều người mặc áo xanh dương trong phòng thí nghiệm\"\n    # query_en = \"a person wearing a helmet riding a motorbike on a street\"\n\n    # Chỉ nửa đầu video:\n    hits = search_frames(\n        video_path, query_vi,\n        every_ms=500, topk=10, time_window=\"first_half\"\n    )\n    for h in hits:\n        print(f\"Rank {h['rank']} | time {h['time_sec']:.2f}s | score {h['similarity']:.3f}\")\n        img = Image.open(h[\"thumb\"])\n        display(img) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T09:15:12.165142Z","iopub.execute_input":"2025-08-22T09:15:12.165439Z","iopub.status.idle":"2025-08-22T09:21:06.623472Z","shell.execute_reply.started":"2025-08-22T09:15:12.165419Z","shell.execute_reply":"2025-08-22T09:21:06.622671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install open-clip-torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:12:10.037024Z","iopub.execute_input":"2025-08-22T08:12:10.037301Z","iopub.status.idle":"2025-08-22T08:13:44.115421Z","shell.execute_reply.started":"2025-08-22T08:12:10.037261Z","shell.execute_reply":"2025-08-22T08:13:44.114288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu\n!pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T08:13:44.117185Z","iopub.execute_input":"2025-08-22T08:13:44.117550Z","iopub.status.idle":"2025-08-22T08:13:55.447971Z","shell.execute_reply.started":"2025-08-22T08:13:44.117484Z","shell.execute_reply":"2025-08-22T08:13:55.447221Z"}},"outputs":[],"execution_count":null}]}