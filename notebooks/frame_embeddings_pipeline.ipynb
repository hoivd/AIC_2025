{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12779183,"sourceType":"datasetVersion","datasetId":8079103},{"sourceId":12858514,"sourceType":"datasetVersion","datasetId":8129235},{"sourceId":545126,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":418001,"modelId":435667}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clone Source Code","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/hein-nkhh/unilm.git\n%cd unilm/beit3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import th∆∞ vi·ªán","metadata":{}},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport time\nimport pickle\nimport torch\nfrom IPython.display import clear_output\nimport os\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nfrom modeling_finetune import beit3_large_patch16_384_retrieval\nfrom PIL import Image\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast\nfrom transformers import XLMRobertaTokenizer\nimport json\nimport cv2\nfrom huggingface_hub import HfApi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:12:10.337038Z","iopub.execute_input":"2025-09-02T10:12:10.337319Z","iopub.status.idle":"2025-09-02T10:12:10.341430Z","shell.execute_reply.started":"2025-09-02T10:12:10.337299Z","shell.execute_reply":"2025-09-02T10:12:10.340581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    force=True,                 # √©p ghi ƒë√® c·∫•u h√¨nh c≈© (r·∫•t quan tr·ªçng trong notebook)\n)\n\nlogger = logging.getLogger(\"Embedd Frame\")\nlogger.info(\"Xin ch√†o\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T09:47:39.860538Z","iopub.execute_input":"2025-09-02T09:47:39.861024Z","iopub.status.idle":"2025-09-02T09:47:39.866482Z","shell.execute_reply.started":"2025-09-02T09:47:39.860998Z","shell.execute_reply":"2025-09-02T09:47:39.865638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C√†i ƒë·∫∑t Device Torch","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# H√†m extract frames","metadata":{}},{"cell_type":"code","source":"def extract_frames_with_opencv(\n    video_path: str, \n    target_height: int = 27, \n    target_width: int = 48, \n    target_fps: float = None,        # th√™m tu·ª≥ ch·ªçn fps\n    show_progressbar: bool = False\n):\n    \"\"\"\n    Extracts frames from a video using OpenCV and returns a list of PIL Images.\n    If target_fps is set, frames will be sampled to match that FPS.\n    \"\"\"\n    logger.info(f\"Opening video: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        logger.error(f\"Failed to open video: {video_path}\")\n        raise ValueError(f\"Failed to open video: {video_path}\")\n\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # N·∫øu c√≥ target_fps th√¨ t√≠nh step\n    if target_fps is not None and target_fps > 0 and video_fps > 0:\n        step = int(round(video_fps / target_fps))\n        logger.info(f\"Video FPS: {video_fps:.2f}, target FPS: {target_fps}, step: {step}\")\n    else:\n        step = 1\n        logger.info(f\"Video FPS: {video_fps:.2f}, using all frames\")\n\n    frames = []\n\n    progress_bar = tqdm(total=total_frames, desc=\"Extracting frames\", unit=\"frame\") if show_progressbar else None\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx % step == 0:   # ch·ªâ l·∫•y frame theo step\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_resized = cv2.resize(frame_rgb, (target_width, target_height))\n            img_pil = Image.fromarray(frame_resized)\n            frames.append(img_pil)\n\n        frame_idx += 1\n        if progress_bar:\n            progress_bar.update(1)\n\n    cap.release()\n    if progress_bar:\n        progress_bar.close()\n    logger.info(f\"Extracted {len(frames)} frames (from {total_frames})\")\n    return frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T15:10:00.408460Z","iopub.execute_input":"2025-09-04T15:10:00.408731Z","iopub.status.idle":"2025-09-04T15:10:00.438234Z","shell.execute_reply.started":"2025-09-04T15:10:00.408704Z","shell.execute_reply":"2025-09-04T15:10:00.437194Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# H√†m Extract Embedding","metadata":{}},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer(\"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3.spm\")\n\n# M√¥ h√¨nh beit_3\nckpt = \"/kaggle/input/beit3_base_retrieval/pytorch/default/2/beit3_large_patch16_384_coco_retrieval.pth\"\nmodel = beit3_large_patch16_384_retrieval(pretrained=False)\nstate_dict = torch.load(ckpt, map_location=device)\nmodel.load_state_dict(state_dict[\"model\"], strict=False)\nmodel = model.to(device)\nmodel.eval()\nclear_output()\n\ntransform  = transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T09:35:28.065289Z","iopub.execute_input":"2025-09-02T09:35:28.065596Z","iopub.status.idle":"2025-09-02T09:35:53.756878Z","shell.execute_reply.started":"2025-09-02T09:35:28.065572Z","shell.execute_reply":"2025-09-02T09:35:53.756139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = []\nids = []\n\nwith torch.no_grad():\n    start = time.time()\n    for image in tqdm(image_paths, desc=\"üîÑ Extracting image embeddings\"):\n        # image = Image.open(img_path).convert(\"RGB\")\n        image_tensor = transform(image).unsqueeze(0).to(device)\n\n        with autocast(): \n            vision_cls, _ = model(image=image_tensor, only_infer=True)\n            vision_norm = F.normalize(vision_cls, p=2, dim=-1)\n\n        embeddings.append(vision_norm.squeeze(0).cpu())   # (D,)\n        ids.append(img_path)\n\n        del image_tensor, vision_cls, vision_norm\n        torch.cuda.empty_cache()\n    end = time.time()\n    logger.info(f'Th·ªùi gian embedd {end-start}')\n\nimage_embeddings = torch.stack(embeddings, dim=0)  # (N,D)\ntorch.save({\"embeddings\": image_embeddings, \"ids\": ids}, \"image_embeddings.pt\")\nprint(\"‚úÖ Saved embeddings:\", image_embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:31:33.410982Z","iopub.execute_input":"2025-09-02T10:31:33.411186Z","iopub.status.idle":"2025-09-02T11:27:52.223670Z","shell.execute_reply.started":"2025-09-02T10:31:33.411172Z","shell.execute_reply":"2025-09-02T11:27:52.223001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# H√†m l∆∞u embeddings","metadata":{}},{"cell_type":"code","source":"class EmbeddingIO:\n    def __init__(self, default_fmt: str = \"pkl\"):\n        \"\"\"\n        Class ƒë·ªÉ l∆∞u / load embeddings.\n        Args:\n            default_fmt (str): ƒë·ªãnh d·∫°ng m·∫∑c ƒë·ªãnh (\"pkl\" ho·∫∑c \"pt\")\n        \"\"\"\n        self.default_fmt = default_fmt.lower()\n\n    def save(self, embeddings, file_path: str, fmt: str = None):\n        \"\"\"\n        L∆∞u embeddings ra file pkl ho·∫∑c pt.\n        Args:\n            embeddings: torch.Tensor ho·∫∑c numpy.ndarray\n            file_path (str): ƒë∆∞·ªùng d·∫´n file (kh√¥ng c·∫ßn ƒëu√¥i)\n            fmt (str): \"pkl\" ho·∫∑c \"pt\". N·∫øu None -> d√πng default_fmt\n        \"\"\"\n        fmt = (fmt or self.default_fmt).lower()\n        if fmt == \"pkl\":\n            with open(file_path + \".pkl\", \"wb\") as f:\n                pickle.dump(embeddings, f)\n            print(f\"‚úÖ Saved {file_path}.pkl\")\n        elif fmt == \"pt\":\n            torch.save(embeddings, file_path + \".pt\")\n            print(f\"‚úÖ Saved {file_path}.pt\")\n        else:\n            raise ValueError(\"fmt ph·∫£i l√† 'pkl' ho·∫∑c 'pt'\")\n\n    def load(self, file_path: str, fmt: str = None):\n        \"\"\"\n        Load embeddings t·ª´ file pkl ho·∫∑c pt.\n        Args:\n            file_path (str): ƒë∆∞·ªùng d·∫´n file (kh√¥ng c·∫ßn ƒëu√¥i)\n            fmt (str): \"pkl\" ho·∫∑c \"pt\". N·∫øu None -> d√πng default_fmt\n        Returns:\n            torch.Tensor ho·∫∑c numpy.ndarray\n        \"\"\"\n        fmt = (fmt or self.default_fmt).lower()\n        if fmt == \"pkl\":\n            with open(file_path + \".pkl\", \"rb\") as f:\n                return pickle.load(f)\n        elif fmt == \"pt\":\n            return torch.load(file_path + \".pt\")\n        else:\n            raise ValueError(\"fmt ph·∫£i l√† 'pkl' ho·∫∑c 'pt'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# H√†m Push Embedding","metadata":{}},{"cell_type":"code","source":"class HFUploader:\n    def __init__(self, token: str = None):\n        \"\"\"\n        Kh·ªüi t·∫°o uploader.\n        N·∫øu c√≥ token -> d√πng ƒë·ªÉ x√°c minh.\n        \"\"\"\n        self.api = HfApi()\n        self.token = token\n\n    def upload_file(self, local_path: str, repo_id: str, path_in_repo: str = None, repo_type: str = \"model\"):\n        \"\"\"\n        Upload m·ªôt file l√™n Hugging Face.\n        \"\"\"\n        if path_in_repo is None:\n            import os\n            path_in_repo = os.path.basename(local_path)\n\n        return self.api.upload_file(\n            path_or_fileobj=local_path,\n            path_in_repo=path_in_repo,\n            repo_id=repo_id,\n            repo_type=repo_type,\n            token=self.token\n        )\n\n    def upload_folder(self, local_folder: str, repo_id: str, repo_type: str = \"dataset\", path_in_repo: str = \"\"):\n        \"\"\"\n        Upload m·ªôt th∆∞ m·ª•c l√™n Hugging Face, c√≥ th·ªÉ ch·ªâ ƒë·ªãnh th∆∞ m·ª•c con trong repo.\n        \"\"\"\n        return self.api.upload_folder(\n            folder_path=local_folder,\n            path_in_repo=path_in_repo,   # th∆∞ m·ª•c con trong repo, vd: \"data/\"\n            repo_id=repo_id,\n            repo_type=repo_type,\n            token=self.token\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Th·ª±c thi","metadata":{}},{"cell_type":"code","source":"image_paths = extract_frames_with_opencv('/kaggle/input/aic-sample-test/videos/L21_V001.mp4',\n                                         show_progressbar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:29:59.649878Z","iopub.execute_input":"2025-09-02T10:29:59.650155Z","iopub.status.idle":"2025-09-02T10:31:33.400587Z","shell.execute_reply.started":"2025-09-02T10:29:59.650137Z","shell.execute_reply":"2025-09-02T10:31:33.400015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"io = EmbeddingIO(default_fmt=\"pkl\")\n\nimport torch\nemb = torch.randn(10, 512)\n\n# L∆∞u d∆∞·ªõi d·∫°ng pickle\nio.save(emb, \"/kaggle/working/L21_V001_embeddings\")\n\n# Load l·∫°i pickle\nloaded_pkl = io.load(\"/kaggle/working/L21_V001_embeddings\")\nprint(type(loaded_pkl), getattr(loaded_pkl, \"shape\", None))\n\n# L∆∞u d∆∞·ªõi d·∫°ng torch\nio.save(emb, \"/kaggle/working/L21_V001_embeddings\", fmt=\"pt\")\n\n# Load l·∫°i torch\nloaded_pt = io.load(\"/kaggle/working/L21_V001_embeddings\", fmt=\"pt\")\nprint(type(loaded_pt), getattr(loaded_pt, \"shape\", None))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}