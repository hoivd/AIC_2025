{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":258497457,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sscarecrow/geminiapiforaic?scriptVersionId=258567893\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T19:11:12.433322Z","iopub.execute_input":"2025-08-27T19:11:12.433607Z","iopub.status.idle":"2025-08-27T19:11:15.501614Z","shell.execute_reply.started":"2025-08-27T19:11:12.433588Z","shell.execute_reply":"2025-08-27T19:11:15.50093Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.173.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.3)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GOOGLE_AI_API_KEY\")\n","metadata":{"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'AIzaSyDY65nxz-QbWjhv9sIR4z343iqMlKNMnKI'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Vietnamese Transcript Topic-based Chunking and Key Points Summarizing\n# Compatible with Kaggle environment\n\nimport os\nimport re\nimport json\nimport time\nfrom typing import List, Dict\nimport google.generativeai as genai\nfrom pathlib import Path\nclass VietnameseTranscriptProcessor:\n    def __init__(self, api_key: str):\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel('gemini-1.5-flash')    \n    def clean_transcript_text(self, text: str) -> str:\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'\\[\\d+:\\d+:\\d+\\]', '', text)\n        text = re.sub(r'Speaker \\d+:', '', text, flags=re.IGNORECASE)\n        text = re.sub(r'Người nói \\d+:', '', text, flags=re.IGNORECASE)\n        text = re.sub(r'\\s*\\.\\s*', '. ', text)\n        text = re.sub(r'\\s*\\?\\s*', '? ', text)\n        text = re.sub(r'\\s*\\!\\s*', '! ', text)\n        text = re.sub(r'\\.{2,}', '.', text)\n        return text.strip()\n    def create_topic_based_chunks(self, text: str) -> List[Dict]:\n        prompt = f\"\"\"\nBạn là một chuyên gia phân tích ngôn ngữ tiếng Việt. Nhiệm vụ của bạn là phân chia văn bản transcript sau thành các đoạn dựa trên chủ đề (topic-based chunking).\n\nHƯỚNG DẪN CHI TIẾT:\n1. Phân tích toàn bộ văn bản để xác định các chủ đề chính\n2. Chia văn bản thành các đoạn, mỗi đoạn tập trung vào MỘT chủ đề cụ thể\n3. Mỗi đoạn phải có ý nghĩa hoàn chỉnh và độc lập\n4. Đảm bảo không bỏ sót nội dung nào từ văn bản gốc\n5. Không thay đổi hoặc chỉnh sửa nội dung, chỉ phân chia theo chủ đề\n6. Nếu có chuyển đổi chủ đề trong câu, hãy cắt tại điểm chuyển đổi phù hợp\n\nCÁC TIÊU CHÍ PHÂN CHIA CHỦ ĐỀ:\n- Thay đổi chủ đề hoặc ý tưởng chính\n- Chuyển từ giới thiệu sang phần chính, từ vấn đề sang giải pháp\n- Thay đổi bối cảnh thời gian hoặc không gian\n- Chuyển từ lý thuyết sang thực hành, từ tổng quan sang chi tiết\n- Chuyển đổi người nói hoặc góc nhìn\n\nVĂN BẢN CẦN PHÂN TÍCH:\n{text}\n\nĐỊNH DẠNG TRẢ VỀ (JSON):\n{{\n  \"chunks\": [\n    {{\n      \"chunk_id\": 1,\n      \"topic\": \"Tên chủ đề chính của đoạn này\",\n      \"content\": \"Nội dung đầy đủ của đoạn văn\",\n      \"description\": \"Mô tả ngắn gọn về nội dung chủ đề\"\n    }},\n    {{\n      \"chunk_id\": 2,\n      \"topic\": \"Tên chủ đề tiếp theo\",\n      \"content\": \"Nội dung đầy đủ của đoạn văn\",\n      \"description\": \"Mô tả ngắn gọn về nội dung chủ đề\"\n    }}\n  ]\n}}\n\nHãy trả về kết quả theo đúng định dạng JSON trên.\n\"\"\"\n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n            if json_match:\n                chunks_data = json.loads(json_match.group())\n                return chunks_data.get(\"chunks\", [])\n            else:\n                print(\"Could not parse JSON response from Gemini API\")\n                return self._fallback_topic_chunking(text)\n        except json.JSONDecodeError as e:\n            print(f\"JSON decode error: {e}\")\n            print(f\"Response text: {response_text[:200]}...\")\n            return self._fallback_topic_chunking(text)\n        except Exception as e:\n            print(f\"Error in topic-based chunking: {e}\")\n            return self._fallback_topic_chunking(text)\n    def _fallback_topic_chunking(self, text: str) -> List[Dict]:\n        print(\"Using fallback chunking method...\")\n        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n        if not paragraphs:\n            sentences = re.split(r'[.!?]+', text)\n            paragraphs = []\n            current_para = \"\"\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if not sentence:\n                    continue\n                if len(current_para + sentence) > 800 and current_para:\n                    paragraphs.append(current_para.strip())\n                    current_para = sentence\n                else:\n                    current_para += \" \" + sentence if current_para else sentence\n            if current_para:\n                paragraphs.append(current_para.strip())\n        \n        chunks = []\n        for i, para in enumerate(paragraphs, 1):\n            chunks.append({\n                \"chunk_id\": i,\n                \"topic\": f\"Chủ đề {i}\",\n                \"content\": para,\n                \"description\": f\"Đoạn văn số {i}\"\n            })\n            \n        return chunks\n    def create_overall_key_points_summary(self, chunks: List[Dict]) -> str:\n        chunks_info = \"\"\n        for chunk in chunks:\n            chunks_info += f\"Chủ đề: {chunk['topic']}\\n\"\n            chunks_info += f\"Mô tả: {chunk['description']}\\n\"\n            chunks_info += f\"Nội dung: {chunk['content'][:300]}...\\n\\n\"\n        prompt = f\"\"\"\nBạn là một chuyên gia tóm tắt nội dung tiếng Việt. Dựa trên các đoạn văn được phân chia theo chủ đề dưới đây, hãy tạo một bản tóm tắt các điểm chính (key points) cho toàn bộ nội dung transcript.\n\nHƯỚNG DẪN TÓM TẮT:\n1. Xác định các ý chính quan trọng nhất từ tất cả các chủ đề\n2. Sắp xếp theo thứ tự logic và mức độ quan trọng\n3. Sử dụng bullet points để trình bày rõ ràng\n4. Mỗi điểm chính nên súc tích nhưng đầy đủ thông tin\n5. Bao gồm cả thông điệp chính và kết luận quan trọng\n6. Sử dụng tiếng Việt tự nhiên và dễ hiểu\n\nCÁC ĐOẠN VĂN THEO CHỦ ĐỀ:\n{chunks_info}\n\nĐỊNH DẠNG TÓM TẮT:\n• [Điểm chính 1]: Mô tả chi tiết điểm chính đầu tiên\n• [Điểm chính 2]: Mô tả chi tiết điểm chính thứ hai  \n• [Điểm chính 3]: Mô tả chi tiết điểm chính thứ ba\n...\n• [Kết luận]: Thông điệp hoặc kết luận quan trọng nhất\n\nHãy tóm tắt các điểm chính:\n\"\"\"\n        try:\n            response = self.model.generate_content(prompt)\n            return response.text.strip()\n        except Exception as e:\n            print(f\"Error in creating key points summary: {e}\")\n            return f\"Không thể tạo tóm tắt điểm chính. Lỗi: {e}\"\n    \n    def process_transcript_file(self, file_path: str) -> Dict:\n        print(f\" Processing file: {file_path}\")\n        try:\n            with open(file_path, 'r', encoding='utf-8') as file:\n                raw_text = file.read()\n        except FileNotFoundError:\n            return {\"error\": f\"File not found: {file_path}\"}\n        except Exception as e:\n            return {\"error\": f\"Could not read file: {e}\"}\n        cleaned_text = self.clean_transcript_text(raw_text)\n        chunks = self.create_topic_based_chunks(cleaned_text)\n        \n        if not chunks:\n            return {\"error\": \"No chunks were created\"}\n        print(f\" Created {len(chunks)} topic-based chunks\")\n        time.sleep(2)\n        print(\" Generating overall key points summary...\")\n        overall_summary = self.create_overall_key_points_summary(chunks)\n        results = {\n            \"file_info\": {\n                \"file_path\": file_path,\n                \"original_length_chars\": len(raw_text),\n                \"cleaned_length_chars\": len(cleaned_text),\n                \"processing_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n            \"chunking_results\": {\n                \"total_chunks\": len(chunks),\n                \"chunking_method\": \"topic-based\",\n                \"chunks\": chunks\n            },\n            \"summary\": {\n                \"type\": \"key_points\",\n                \"language\": \"vietnamese\",\n                \"content\": overall_summary\n            }\n        }\n        \n        return results\n    def save_results_to_json(self, results: Dict, output_path: str = \"vietnamese_transcript_analysis.json\"):\n        try:\n            with open(output_path, 'w', encoding='utf-8') as file:\n                json.dump(results, file, ensure_ascii=False, indent=2)\n            print(f\" Results saved to: {output_path}\")\n        except Exception as e:\n            print(f\"❌ Error saving results: {e}\")\ndef main():\n    API_KEY = secret_value_0   \n    if not API_KEY:\n        print(\"❌ API key not found!\")\n        return\n    transcript_file_path = \"/kaggle/input/phowhisperforaic/transcripts/L22_V002.txt\"  # Change this to your file path\n    if not os.path.exists(transcript_file_path):\n        print(f\"❌ Transcript file not found: {transcript_file_path}\")\n        return\n    try:\n        processor = VietnameseTranscriptProcessor(API_KEY)\n    except Exception as e:\n        print(f\"❌ Failed to initialize Gemini API: {e}\")\n        return\n    print(\"\\n🚀 Starting Vietnamese transcript processing...\")\n    results = processor.process_transcript_file(transcript_file_path)\n    if \"error\" in results:\n        print(f\"❌ Processing failed: {results['error']}\")\n        return\n    output_file = \"vietnamese_transcript_analysis.json\"\n    processor.save_results_to_json(results, output_file)\n    print(\"\\n\" + \"PROCESSING RESULTS\" + \"\\n\" + \"=\" * 50)\n    print(f\"Topic-based chunks: {results['chunking_results']['total_chunks']}\")\n    print(f\"\\n IDENTIFIED TOPICS:\")\n    print(\"-\" * 30)\n    for i, chunk in enumerate(results['chunking_results']['chunks'], 1):\n        print(f\"{i:2d}. {chunk['topic']}\")\n        print(f\"    └─ {chunk['description']}\")\n    print(f\"\\n KEY POINTS SUMMARY:\")\n    print(\"-\" * 30)\n    print(results['summary']['content'])\n    \n    print(f\"\\n Processing completed successfully!\")\n    print(f\" Full results saved in: {output_file}\")\n    \n    return results\ndef process_file_directly(file_path: str, api_key: str) -> Dict:\n    processor = VietnameseTranscriptProcessor(api_key)\n    return processor.process_transcript_file(file_path)\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T19:31:14.32907Z","iopub.execute_input":"2025-08-27T19:31:14.32997Z","iopub.status.idle":"2025-08-27T19:32:06.220595Z","shell.execute_reply.started":"2025-08-27T19:31:14.329943Z","shell.execute_reply":"2025-08-27T19:32:06.219922Z"}},"outputs":[{"name":"stdout","text":"\n🚀 Starting Vietnamese transcript processing...\n Processing file: /kaggle/input/phowhisperforaic/transcripts/L22_V002.txt\n Created 16 topic-based chunks\n Generating overall key points summary...\n Results saved to: vietnamese_transcript_analysis.json\n\nPROCESSING RESULTS\n==================================================\nTopic-based chunks: 16\n\n IDENTIFIED TOPICS:\n------------------------------\n 1. Ngày hội việc làm ngành y tế\n    └─ Thông tin về ngày hội việc làm ngành y tế, số lượng bác sĩ được tuyển dụng và sự phân bổ của họ tại các tuyến y tế.\n 2. Công bố điểm chuẩn và kết quả xét tuyển đại học\n    └─ Thông tin về thời gian và quy trình công bố điểm chuẩn, kết quả xét tuyển đại học và các quy định liên quan.\n 3. Cứu sống thai phụ mắc thuyên tắc mạch phổi\n    └─ Thông tin về trường hợp cứu sống thành công một thai phụ mắc thuyên tắc mạch phổi nặng tại bệnh viện Chợ Rẫy.\n 4. Kiểm tra tiến độ dự án hạ tầng giao thông\n    └─ Phó Chủ tịch UBND TP. HCM kiểm tra và yêu cầu các đơn vị khẩn trương khắc phục khó khăn, đảm bảo tiến độ thi công các dự án hạ tầng giao thông trọng điểm.\n 5. Khởi công dự án nâng cấp đường Thạnh Xuân 25 và nạo vét bờ bao\n    └─ Thông tin về lễ khởi công dự án nâng cấp, mở rộng đường Thạnh Xuân 25 và nạo vét, kiên cố hóa bờ bao tại Quận 12, TP.HCM.\n 6. Trung Quốc hứng chịu mưa lũ dữ dội\n    └─ Thông tin về tình hình mưa lũ dữ dội tại tỉnh Hồ Nam, Trung Quốc, gây thiệt hại nghiêm trọng về người và tài sản.\n 7. Biểu tình của nhân viên ngành game vì lo ngại AI\n    └─ Hơn 300 nhân viên ngành game tại Mỹ biểu tình phản đối việc các công ty sử dụng AI để thay thế công việc của họ mà không có sự đền bù hợp lý.\n 8. Ảnh hưởng của việc sử dụng smartphone đến sức khỏe tinh thần của thiếu niên\n    └─ Nghiên cứu chỉ ra rằng việc sử dụng smartphone thiếu kiểm soát ở thiếu niên có liên hệ đến rối loạn lo âu và trầm cảm.\n 9. Dự án sử dụng chó để gieo hạt giống tái tạo môi trường\n    └─ Mô tả dự án sử dụng chó để gieo hạt giống nhằm tái tạo môi trường tự nhiên tại Anh, lấy cảm hứng từ loài chó sói và kinh nghiệm của hai phụ nữ ở Chile.\n10. Olympic Paris: Du lịch vòng quanh thế giới tại Paris\n    └─ Du khách đến Paris trong thời gian diễn ra Olympic có thể trải nghiệm văn hóa của nhiều quốc gia khác nhau ngay tại Paris.\n11. Lễ hội quốc tế Edinburgh\n    └─ Thông tin về Lễ hội quốc tế Edinburgh, diễn ra tại Scotland vào tháng 8 hàng năm, với chủ đề về nghi thức và sự tham gia của nhiều nghệ sĩ quốc tế.\n12. Các vụ tai nạn giao thông và đuối nước\n    └─ Thông tin về một số vụ tai nạn giao thông, đuối nước, cháy nhà trọ và các vụ việc khác đang được điều tra làm rõ.\n13. Tai nạn lao động tại khu công nghiệp Tần Lõm\n    └─ Thông tin về vụ tai nạn lao động tại khu công nghiệp Tần Lõm, Lào Cai, khiến 1 người chết và 5 người bị thương.\n14. Khởi tố vụ án vi phạm quy định về đấu thầu\n    └─ Thông tin về việc khởi tố bị can và bắt tạm giam Giám đốc Sở NN&PTNT TP.HCM và Giám đốc Công ty Vạn Phát về tội vi phạm quy định về đấu thầu.\n15. Kết thúc chương trình\n    └─ Phần kết thúc chương trình.\n16. Thông tin không rõ nguồn gốc\n    └─ Câu này không rõ nguồn gốc và không liên quan đến các chủ đề khác trong bản tin.\n\n KEY POINTS SUMMARY:\n------------------------------\n• **Công bố điểm chuẩn và kết quả xét tuyển đại học:**  Bộ Giáo dục và Đào tạo sẽ công bố điểm chuẩn và kết quả xét tuyển đại học từ chiều ngày 17 tháng 8.\n\n• **Cứu sống thai phụ mắc thuyên tắc mạch phổi:** Bệnh viện Chợ Rẫy cứu sống thành công một thai phụ mắc thuyên tắc mạch phổi nặng, nguy cơ tử vong ban đầu lên đến 80%.\n\n• **Kiểm tra tiến độ dự án hạ tầng giao thông TP.HCM:** Phó Chủ tịch UBND TP.HCM yêu cầu các đơn vị khẩn trương khắc phục khó khăn, đảm bảo tiến độ thi công các dự án hạ tầng giao thông trọng điểm.\n\n• **Khởi công dự án nâng cấp đường Thạnh Xuân 25 và nạo vét bờ bao:** Quận 12, TP.HCM khởi công dự án nâng cấp, mở rộng đường Thạnh Xuân 25 và nạo vét, kiên cố hóa bờ bao nhằm giảm ùn tắc và ngập nước.\n\n• **Trung Quốc hứng chịu mưa lũ dữ dội:** Mưa lũ dữ dội tại tỉnh Hồ Nam, Trung Quốc gây ra thiệt hại nghiêm trọng về người và tài sản, với 83 người chết và mất tích.\n\n• **Biểu tình của nhân viên ngành game tại Mỹ:** Hơn 300 nhân viên ngành game tại Mỹ biểu tình phản đối việc sử dụng AI thay thế công việc mà không có đền bù hợp lý.\n\n• **Ảnh hưởng của smartphone đến sức khỏe tinh thần thiếu niên:** Nghiên cứu chỉ ra rằng sử dụng smartphone thiếu kiểm soát ở thiếu niên có liên hệ đến rối loạn lo âu và trầm cảm.\n\n• **Dự án sử dụng chó để gieo hạt giống tái tạo môi trường:** Dự án tại Anh sử dụng chó để gieo hạt giống nhằm tái tạo môi trường tự nhiên.\n\n• **Olympic Paris: trải nghiệm văn hóa đa quốc gia:** Du khách đến Paris trong thời gian Olympic có thể trải nghiệm văn hóa của nhiều quốc gia khác nhau.\n\n• **Lễ hội quốc tế Edinburgh:** Lễ hội quốc tế Edinburgh diễn ra tại Scotland vào tháng 8, với chủ đề về nghi thức và sự tham gia của nhiều nghệ sĩ quốc tế.\n\n• **Các vụ tai nạn giao thông, đuối nước và các vụ việc khác:** Bản tin cập nhật về một số vụ tai nạn giao thông, đuối nước, cháy nhà trọ và các vụ việc khác đang được điều tra.\n\n• **Tai nạn lao động tại khu công nghiệp Tần Lõm:** Vụ tai nạn lao động tại khu công nghiệp Tần Lõm, Lào Cai khiến 1 người chết và 5 người bị thương.\n\n• **Khởi tố vụ án vi phạm quy định về đấu thầu:** Giám đốc Sở NN&PTNT TP.HCM và Giám đốc Công ty Vạn Phát bị khởi tố và bắt tạm giam về tội vi phạm quy định về đấu thầu.\n\n\n• **Kết luận:** Bản tin cập nhật nhiều thông tin đa dạng về các sự kiện xã hội, kinh tế, y tế và môi trường trong nước và quốc tế, nhấn mạnh tầm quan trọng của an toàn giao thông, sức khỏe cộng đồng và công tác phòng chống thiên tai.\n\n Processing completed successfully!\n Full results saved in: vietnamese_transcript_analysis.json\n","output_type":"stream"}],"execution_count":27}]}